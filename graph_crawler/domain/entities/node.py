"""Базовий клас для вузла графу (веб-сторінки) - Pydantic модель."""

import asyncio
import logging
import os
import uuid
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from typing import TYPE_CHECKING, Any, Dict, List, Optional, Protocol, Tuple

from pydantic import BaseModel, ConfigDict, Field, field_validator

# Це вирішує circular imports через Dependency Inversion Principle
from graph_crawler.domain.interfaces.node_interfaces import (
    INodePluginContext,
    INodePluginType,
    IPluginManager,
)
from graph_crawler.domain.value_objects.lifecycle import (
    NodeLifecycle,
    NodeLifecycleError,
)
from graph_crawler.infrastructure.adapters.base import BaseTreeAdapter

if TYPE_CHECKING:
    from graph_crawler.extensions.plugins.node import NodePluginContext, NodePluginType

logger = logging.getLogger(__name__)

# ============ THREAD POOL для HTML PARSING ============
# Використовуємо ThreadPoolExecutor для переносу блокуючих операцій парсингу HTML
# з main event loop в окремі потоки
_html_executor = ThreadPoolExecutor(max_workers=os.cpu_count() or 4)


# ============ АБСТРАКЦІЇ ============
# Dependency Inversion Principle: Node залежить від абстракцій, не конкретних реалізацій


class ITreeAdapter(Protocol):
    """
    Інтерфейс для Tree Adapter (Protocol для DIP).

    Node залежить від цього інтерфейсу, а не від конкретного BaseTreeAdapter.

    Example:
        >>> class CustomAdapter:
        ...     def parse(self, html: str):
        ...         return custom_tree
        >>> node = Node(url="...", tree_parser=CustomAdapter())
    """

    def parse(self, html: str) -> Any:
        """Парсить HTML в дерево."""
        ...


class IContentHashStrategy(Protocol):
    """
    Інтерфейс для обчислення content hash (Protocol).

        Забезпечує дотримання Liskov Substitution Principle:
        - Чітко визначений контракт: метод повертає строку (SHA256 hex digest)
        - Користувач може створити власну стратегію, не порушуючи контракт
        - Валідація результату гарантує що це валідний хеш

        Example:
            >>> class CustomHashStrategy:
            ...     def compute_hash(self, node: 'Node') -> str:
            ...         # Повертає SHA256 hex digest (64 символи)
            ...         return hashlib.sha256(node.metadata['h1'].encode()).hexdigest()
            >>>
            >>> node.hash_strategy = CustomHashStrategy()
            >>> hash_value = node.get_content_hash()  # Використає кастомну стратегію
    """

    def compute_hash(self, node: "Node") -> str:
        """
        Обчислює content hash для ноди.

        Контракт:
        - MUST повертати SHA256 hex digest (64 символи, lowercase)
        - MUST бути детермінованим (однакові дані → однаковий хеш)
        - MUST викликатися тільки після process_html() (HTML_STAGE)

        Args:
            node: Node для якої обчислюється хеш

        Returns:
            SHA256 hex digest string (64 символи)
        """
        ...


class Node(BaseModel):
    """
    Базовий клас для вузла графу (веб-сторінка) - Pydantic модель.

    Кожен вузол представляє одну веб-сторінку з метаданими.
    Користувачі можуть успадковувати цей клас для додавання власної логіки.

    ВАЖЛИВО: HTML не зберігається у пам'яті для економії RAM.
    HTML обробляється одразу та видаляється.

     ЖИТТЄВИЙ ЦИКЛ NODE (2 ЧІТКІ ЕТАПИ):

    ЕТАП 1: СТВОРЕННЯ - URL_STAGE (__init__)
        Доступно: url, depth, should_scan, can_create_edges
        Що можна:
           * Аналізувати URL на ключові слова
           * Визначати параметри по домену
           * Встановлювати should_scan, can_create_edges
           * Викликати on_node_created хуки
         Що НЕМОЖНА:
           * Працювати з HTML (його ще немає!)
           * Витягувати метадані (їх ще немає!)
           * Аналізувати контент сторінки

    ЕТАП 2: ОБРОБКА HTML - HTML_STAGE (process_html)
         INPUT (на початку process_html):
           * html - HTML контент (string)
           * html_tree - DOM дерево (після парсингу)
           * parser - Tree adapter для роботи з деревом

         ОБРОБКА (через плагіни):
           * MetadataExtractorPlugin витягує title, h1, description, keywords
           * LinkExtractorPlugin витягує посилання <a href>
           * CustomPlugins - ваша власна логіка

         OUTPUT (після process_html):
           * metadata - заповнені метадані (dict)
           * user_data - дані від плагінів (dict)
           * extracted_links - список URL (list)
           * HTML та html_tree ВИДАЛЕНІ з пам'яті!

        Що можна:
           * Витягувати метадані через плагіни
           * Аналізувати текст сторінки
           * Шукати ключові слова в контенті
           * Витягувати посилання
           * Виконувати плагіни
         Що НЕМОЖНА:
           * Змінювати базові параметри (url, depth)

    Розділення етапів запобігає помилкам:
     Пошук ключових слів в HTML до сканування
     Використання metadata при створенні ноди
     Виклик методів не на своєму етапі

    Атрибути:
        url: URL сторінки
        node_id: Унікальний ідентифікатор вузла
        metadata: Додаткові метадані (заповнюються плагінами після process_html)
        scanned: Чи була сторінка просканована
        should_scan: Чи треба сканувати цей вузол (False для зовнішніх посилань)
        can_create_edges: Чи може вузол створювати нові зв'язки (edges)
        depth: Глибина вузла від кореневого
        created_at: Час створення вузла
        response_status: HTTP статус код відповіді
        lifecycle_stage: Поточний етап життєвого циклу
        user_data: Додаткові дані від користувача (кастомні поля, заповнюються плагінами)
    """

    # ============ PYDANTIC FIELDS ============
    # Базові параметри (ЕТАП 1: URL_STAGE)
    url: str
    node_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    depth: int = Field(default=0, ge=0)
    should_scan: bool = True
    can_create_edges: bool = True
    created_at: datetime = Field(default_factory=datetime.now)

    # Параметри для ЕТАП 2: HTML_STAGE
    metadata: Dict[str, Any] = Field(default_factory=dict)
    user_data: Dict[str, Any] = Field(default_factory=dict)
    scanned: bool = False
    response_status: Optional[int] = None

    # Incremental Crawling - content hash (обчислюється після process_html)
    content_hash: Optional[str] = None

    # Scheduler перевіряє це поле ПЕРЕД URLRule (див. scheduler.py)
    priority: Optional[int] = Field(default=None, ge=1, le=10)

    # Lifecycle
    lifecycle_stage: NodeLifecycle = NodeLifecycle.URL_STAGE

    # Plugin Manager (не серіалізується)
    # Використовуємо Any замість Protocol для сумісності з Pydantic
    plugin_manager: Optional[Any] = Field(default=None, exclude=True)

    # Tree Parser/Adapter (не серіалізується)
    tree_parser: Optional[Any] = Field(default=None, exclude=True)

    hash_strategy: Optional[Any] = Field(default=None, exclude=True)

    # Pydantic configuration
    model_config = ConfigDict(
        arbitrary_types_allowed=True,  # Для NodePluginManager, BeautifulSoup
        validate_assignment=True,  # Валідація при присвоєнні
        use_enum_values=False,  # Зберігаємо enum об'єкти
    )

    @field_validator("url")
    @classmethod
    def validate_url(cls, v: str) -> str:
        """Валідація URL."""
        from urllib.parse import urlparse

        from graph_crawler.shared.exceptions import InvalidURLError

        if not v:
            raise InvalidURLError("URL cannot be empty")

        if not v.startswith(("http://", "https://")):
            raise InvalidURLError(f"URL must start with http:// or https://, got: {v}")

        parsed = urlparse(v)
        if not parsed.netloc:
            raise InvalidURLError(f"URL must have a valid domain: {v}")

        return v

    def model_post_init(self, __context: Any) -> None:
        """Викликається після ініціалізації моделі Pydantic."""
        # Викликаємо хук ON_NODE_CREATED (ЕТАП 1)
        self._trigger_node_created_hook()

    def _trigger_node_created_hook(self):
        """
        Викликає хук ON_NODE_CREATED після створення ноди.

        Це ЕТАП 1 - доступний ТІЛЬКИ URL.
        Користувач може додати свою логіку через плагіни.
        """
        if not self.plugin_manager:
            return

        # Lazy import конкретної реалізації - це НОРМАЛЬНО і є частиною рішення.
        # Node залежить від Protocol interfaces (INodePluginContext, IPluginManager),
        # а конкретні реалізації (NodePluginContext) імпортуються lazy для уникнення circular deps.
        from graph_crawler.extensions.plugins.node import (
            NodePluginContext,
            NodePluginType,
        )

        # Створюємо контекст для плагінів
        context = NodePluginContext(
            node=self,
            url=self.url,
            depth=self.depth,
            should_scan=self.should_scan,
            can_create_edges=self.can_create_edges,
        )

        # Виконуємо плагіни ON_NODE_CREATED (sync)
        context = self.plugin_manager.execute_sync(
            NodePluginType.ON_NODE_CREATED, context
        )

        # Оновлюємо параметри з контексту (користувач міг їх змінити)
        self.should_scan = context.should_scan
        self.can_create_edges = context.can_create_edges
        self.user_data.update(context.user_data)

    async def process_html(self, html: str) -> List[str]:
        """
        ============ ЕТАП 2: HTML_STAGE ============

        Async обробляє HTML через ЧИСТУ ПЛАГІННУ СИСТЕМУ. Тепер async для підтримки async плагінів (ML, LLM, API).

        Args:
            html: HTML контент сторінки

        Returns:
            Список знайдених URL посилань

        Raises:
            NodeLifecycleError: Якщо нода вже просканована
        """
        # Перевірка lifecycle
        if self.lifecycle_stage == NodeLifecycle.HTML_STAGE:
            logger.warning(f"Node already processed: {self.url}")
            return []

        # Переходимо на ЕТАП 2
        self.lifecycle_stage = NodeLifecycle.HTML_STAGE

        # Крок 1: Парсинг HTML (ASYNC через ThreadPoolExecutor для швидкості)
        parser, html_tree = await self._parse_html_async(html)

        # Крок 2: Створення контексту та виконання плагінів (async)
        # ПРИМІТКА: _update_from_context тепер викликається всередині _execute_plugins
        # перед ON_AFTER_SCAN плагінами, щоб кастомні Node класи могли заповнити поля
        context = await self._execute_plugins(html, html_tree, parser)

        # Крок 3: Фінальне оновлення user_data з контексту (для змін від ON_AFTER_SCAN плагінів)
        # metadata вже оновлено в _execute_plugins
        self.user_data.update(context.user_data)

        # Крок 4: Обчислення hash
        self._compute_content_hash()

        # Крок 5: Очищення пам'яті
        self._cleanup_memory(html, html_tree, context)

        logger.debug(
            f"Processed HTML for {self.url}: {len(context.extracted_links)} links, metadata keys: {list(self.metadata.keys())}"
        )

        return context.extracted_links

    def _parse_html_sync(self, html: str) -> Tuple[Any, Any]:
        """
        Синхронний парсинг HTML в дерево через adapter.
        
        Використовується через ThreadPoolExecutor для не блокування event loop.
        
        Args:
            html: HTML контент
            
        Returns:
            Tuple (parser, html_tree)
        """
        if self.tree_parser is None:
            from graph_crawler.infrastructure.adapters import get_default_parser

            parser = get_default_parser()
        else:
            parser = self.tree_parser

        html_tree = parser.parse(html)
        return parser, html_tree
    
    async def _parse_html_async(self, html: str) -> Tuple[Any, Any]:
        """
        Async парсинг HTML через ThreadPoolExecutor.
        
        ОПТИМІЗАЦІЯ: BeautifulSoup парсинг є CPU-bound операцією і блокує event loop.
        Переносимо її в ThreadPoolExecutor для паралельної обробки.
        
        Args:
            html: HTML контент
            
        Returns:
            Tuple (parser, html_tree)
        """
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            _html_executor,
            self._parse_html_sync,
            html
        )

    async def _execute_plugins(self, html: str, html_tree: Any, parser: Any) -> Any:
        """
        Async виконує плагіни для обробки HTML. Тепер async для підтримки async плагінів (ML, LLM, API).

        Args:
            html: HTML контент
            html_tree: Парсоване дерево
            parser: Tree adapter

        Returns:
            NodePluginContext з результатами
        """
        # Lazy import конкретної реалізації - це НОРМАЛЬНО і є частиною рішення.
        # Node залежить від Protocol interfaces для type hints, але використовує
        # конкретні реалізації через lazy import для уникнення circular deps.
        from graph_crawler.extensions.plugins.node import (
            NodePluginContext,
            NodePluginType,
        )

        # Створюємо контекст
        context = NodePluginContext(
            node=self,
            url=self.url,
            depth=self.depth,
            should_scan=self.should_scan,
            can_create_edges=self.can_create_edges,
            html=html,
            html_tree=html_tree,
            parser=parser,
            metadata=self.metadata.copy(),
            user_data=self.user_data.copy(),
        )

        if self.plugin_manager:
            context = await self.plugin_manager.execute(
                NodePluginType.ON_BEFORE_SCAN, context
            )
            context = await self.plugin_manager.execute(
                NodePluginType.ON_HTML_PARSED, context
            )

            # Оновлюємо ноду ПЕРЕД виконанням ON_AFTER_SCAN плагінів
            # Це дозволяє кастомним Node класам заповнити поля (наприклад, text)
            # які потім будуть використані плагінами (наприклад, RealTimeVectorizerPlugin)
            self._update_from_context(context)

            context = await self.plugin_manager.execute(
                NodePluginType.ON_AFTER_SCAN, context
            )

        return context

    def _update_from_context(self, context: Any):
        """
        Оновлює ноду результатами з контексту.

        Args:
            context: NodePluginContext з результатами плагінів
        """
        self.metadata = context.metadata
        self.user_data.update(context.user_data)

    def _compute_content_hash(self):
        """
        Обчислює content hash для Incremental Crawling.
        """
        try:
            self.content_hash = self.get_content_hash()
            logger.debug(
                f"Content hash computed for {self.url}: {self.content_hash[:16]}..."
            )
        except Exception as e:
            logger.warning(f"Failed to compute content_hash for {self.url}: {e}")
            self.content_hash = None

    def _cleanup_memory(self, html: str, html_tree: Any, context: Any):
        """
        Видаляє HTML та дерево з пам'яті (критично для 20k+ сторінок).

        Args:
            html: HTML контент
            html_tree: Парсоване дерево
            context: NodePluginContext
        """
        del html
        del html_tree
        context.html = None
        context.html_tree = None

    def get_content_hash(self) -> str:
        """
                Обчислює hash контенту для детекції змін (Incremental Crawling).

        Використовує IContentHashStrategy Protocol для гарантії контракту.

                ДЕФОЛТНА РЕАЛІЗАЦІЯ: SHA256 від чистого тексту сторінки.

                КОРИСТУВАЧ МОЖЕ ЗАДАТИ КАСТОМНУ СТРАТЕГІЮ через hash_strategy:

                Example 1: Кастомна стратегія (рекомендовано)
                    >>> class H1HashStrategy:
                    ...     def compute_hash(self, node):
                    ...         return hashlib.sha256(node.metadata['h1'].encode()).hexdigest()
                    >>>
                    >>> node.hash_strategy = H1HashStrategy()
                    >>> hash_value = node.get_content_hash()

                Example 2: Наслідування (альтернатива)
                    >>> class MyNode(Node):
                    ...     def get_content_hash(self):
                    ...         if self.hash_strategy:
                    ...             return self.hash_strategy.compute_hash(self)
                    ...         return hashlib.sha256(self.metadata['h1'].encode()).hexdigest()

                ВАЖЛИВО: Можна викликати ТІЛЬКИ після process_html() (ЕТАП 2: HTML_STAGE).

                Returns:
                    SHA256 hex digest string (64 символи, lowercase)

                Raises:
                    NodeLifecycleError: Якщо викликано до process_html()
                    ValueError: Якщо hash_strategy повертає невалідний хеш
        """
        import hashlib
        import re

        # Перевірка lifecycle - можна викликати тільки після process_html
        if self.lifecycle_stage != NodeLifecycle.HTML_STAGE:
            raise NodeLifecycleError(
                f"Cannot compute content_hash at {self.lifecycle_stage.value}. "
                f"Call process_html() first (must be at HTML_STAGE)."
            )

        # Якщо задана кастомна стратегія - використовуємо її
        if self.hash_strategy:
            hash_value = self.hash_strategy.compute_hash(self)

            # LSP: Валідація що результат - це валідний SHA256 хеш
            if not isinstance(hash_value, str):
                raise ValueError(
                    f"Hash strategy must return string, got {type(hash_value).__name__}. "
                    f"Strategy: {type(self.hash_strategy).__name__}"
                )

            from graph_crawler.shared.constants import (
                SHA256_HASH_LENGTH,
                SHA256_HASH_PATTERN,
            )

            if not re.match(SHA256_HASH_PATTERN, hash_value):
                raise ValueError(
                    f"Hash strategy must return valid SHA256 hex digest ({SHA256_HASH_LENGTH} chars, lowercase), "
                    f"got: '{hash_value[:20]}...' (len={len(hash_value)}). "
                    f"Strategy: {type(self.hash_strategy).__name__}"
                )

            # LSP: Валідація детермінованості стратегії
            # Перевірка виконується тільки один раз при першому виклику
            if not hasattr(self, "_hash_determinism_validated"):
                self._validate_hash_strategy_deterministic(hash_value)
                self._hash_determinism_validated = True

            return hash_value

        # Дефолтна стратегія - hash від чистого тексту сторінки
        from graph_crawler.shared.constants import DEFAULT_HASH_ENCODING

        text = self.user_data.get("text_content", "")
        return hashlib.sha256(text.encode(DEFAULT_HASH_ENCODING)).hexdigest()

    def _validate_hash_strategy_deterministic(self, first_hash: str) -> None:
        """
        Перевіряє чи hash_strategy детермінована (LSP Principle).

        Викликає стратегію двічі з тими самими даними і перевіряє чи хеші ідентичні.
        Це критично для incremental crawling, бо недетермінована стратегія
        призведе до хибних спрацювань change detection.

        Args:
            first_hash: Перший обчислений хеш для порівняння

        Raises:
            ValueError: Якщо стратегія недетермінована (повертає різні хеші)

        Warning:
            Цей метод викликається тільки один раз при першому обчисленні хешу.
            Якщо стратегія використовує зовнішні змінні (час, випадкові числа),
            тест може давати false positive.
        """
        if not self.hash_strategy:
            return

        # Викликаємо стратегію другий раз з тими самими даними
        second_hash = self.hash_strategy.compute_hash(self)

        if first_hash != second_hash:
            raise ValueError(
                f"Hash strategy is NOT DETERMINISTIC! "
                f"Got different hashes for same data:\n"
                f"  1st call: {first_hash[:32]}...\n"
                f"  2nd call: {second_hash[:32]}...\n"
                f"Strategy: {type(self.hash_strategy).__name__}\n\n"
                f"This will break incremental crawling! "
                f"Hash strategy MUST return same hash for same input data."
            )

    def mark_as_scanned(self):
        """Позначає вузол як просканований."""
        self.scanned = True

    def model_dump(self, **kwargs) -> Dict[str, Any]:
        """
        Серіалізує вузол у словник.

        Pydantic автоматично:
        - Серіалізує всі поля (включаючи кастомні в підкласах)
        - Виключає plugin_manager (exclude=True)
        - Конвертує datetime, enum в JSON-compatible формати
        """
        data = super().model_dump(**kwargs)

        # Конвертуємо lifecycle_stage в string для JSON
        if "lifecycle_stage" in data and isinstance(
            data["lifecycle_stage"], NodeLifecycle
        ):
            data["lifecycle_stage"] = data["lifecycle_stage"].value

        # Конвертуємо datetime в ISO format
        if "created_at" in data and isinstance(data["created_at"], datetime):
            data["created_at"] = data["created_at"].isoformat()

        return data

    @classmethod
    def model_validate(
        cls, obj: Any, context: Optional[Dict] = None, **kwargs
    ) -> "Node":
        """
        Десеріалізує вузол зі словника або JSON.

        Pydantic автоматично:
        - Валідує всі поля
        - Конвертує типи (str -> datetime, str -> enum)
        - Підтримує кастомні поля в підкласах

        Args:
            obj: Об'єкт для валідації
            context: Опціональний контекст з залежностями (plugin_manager, tree_parser)
            **kwargs: Додаткові параметри для Pydantic

        Returns:
            Валідований Node об'єкт з відновленими залежностями

        Example:
            >>> from graph_crawler.infrastructure.adapters.beautifulsoup_adapter import BeautifulSoupAdapter
            >>> context = {
            ...     'plugin_manager': NodePluginManager(),
            ...     'tree_parser': BeautifulSoupAdapter()
            ... }
            >>> node = Node.model_validate(node_dict, context=context)
        """
        # Якщо це словник, конвертуємо lifecycle_stage та created_at
        if isinstance(obj, dict):
            if "lifecycle_stage" in obj and isinstance(obj["lifecycle_stage"], str):
                obj["lifecycle_stage"] = NodeLifecycle(obj["lifecycle_stage"])

            if "created_at" in obj and isinstance(obj["created_at"], str):
                obj["created_at"] = datetime.fromisoformat(obj["created_at"])

        node = super().model_validate(obj, **kwargs)

        # Відновлюємо залежності з контексту якщо передані
        #  ВАЖЛИВО: plugin_manager та tree_parser не серіалізуються
        # Вони мають бути передані через context при десеріалізації
        if context:
            if "plugin_manager" in context:
                node.plugin_manager = context["plugin_manager"]
            if "tree_parser" in context:
                node.tree_parser = context["tree_parser"]

        return node

    def restore_dependencies(
        self,
        plugin_manager: Optional[IPluginManager] = None,
        tree_parser: Optional[ITreeAdapter] = None,
        hash_strategy: Optional[IContentHashStrategy] = None,
    ):
        """
                Відновлює залежності після десеріалізації.

                 ВАЖЛИВО: plugin_manager, tree_parser та hash_strategy не серіалізуються.
                Після завантаження Node з JSON/SQLite, ці поля будуть None.
                Використовуйте цей метод для відновлення залежностей.

        Приймає будь-який об'єкт що реалізує Protocol (не тільки конкретні класи)
        Додано hash_strategy для кастомізації обчислення hash

                Args:
                    plugin_manager: Будь-який об'єкт з методом execute() (IPluginManager Protocol)
                    tree_parser: Будь-який об'єкт з методом parse() (ITreeAdapter Protocol)
                    hash_strategy: Будь-який об'єкт з методом compute_hash() (IContentHashStrategy Protocol)

                Example:
                    >>> from graph_crawler.extensions.plugins.node import NodePluginManager
                    >>> from graph_crawler.infrastructure.adapters.beautifulsoup_adapter import BeautifulSoupAdapter
                    >>>
                    >>> node = Node.model_validate(node_dict)
                    >>> node.restore_dependencies(
                    ...     plugin_manager=NodePluginManager(),
                    ...     tree_parser=BeautifulSoupAdapter(),
                    ...     hash_strategy=CustomHashStrategy()
                    ... )
        """
        if plugin_manager is not None:
            self.plugin_manager = plugin_manager
        if tree_parser is not None:
            self.tree_parser = tree_parser
        if hash_strategy is not None:
            self.hash_strategy = hash_strategy

    # =====================================
    # LAW OF DEMETER: Методи-обгортки для metadata
    # =====================================
    # Замість node.metadata.get("title") використовуємо node.get_title()

    def _get_metadata_field(self, field: str, default: Any = None) -> Any:
        """
                Універсальний helper для отримання полів з metadata.

        Усуває дублювання коду в 6+ getter методах.
                Централізує логіку доступу до metadata для дотримання DRY принципу.

                Args:
                    field: Назва поля в metadata
                    default: Значення за замовчуванням якщо поле не знайдено

                Returns:
                    Значення поля або default
        """
        return self.metadata.get(field, default) if self.metadata else default

    def get_title(self) -> Optional[str]:
        """Отримати title сторінки (Law of Demeter wrapper)."""
        return self._get_metadata_field("title")

    def get_description(self) -> Optional[str]:
        """Отримати description сторінки (Law of Demeter wrapper)."""
        return self._get_metadata_field("description")

    def get_h1(self) -> Optional[str]:
        """Отримати H1 заголовок сторінки (Law of Demeter wrapper)."""
        return self._get_metadata_field("h1")

    def get_keywords(self) -> Optional[str]:
        """Отримати keywords сторінки (Law of Demeter wrapper)."""
        return self._get_metadata_field("keywords")

    def get_canonical_url(self) -> Optional[str]:
        """Отримати canonical URL сторінки (Law of Demeter wrapper)."""
        return self._get_metadata_field("canonical_url")

    def get_language(self) -> Optional[str]:
        """Отримати мову сторінки (Law of Demeter wrapper)."""
        return self._get_metadata_field("language")

    def get_meta_value(self, key: str, default: Any = None) -> Any:
        """Отримати значення з metadata за ключем (Law of Demeter wrapper)."""
        return self._get_metadata_field(key, default)

    def __repr__(self):
        return (
            f"Node(url={self.url}, lifecycle={self.lifecycle_stage.value}, "
            f"scanned={self.scanned}, should_scan={self.should_scan}, "
            f"can_create_edges={self.can_create_edges}, depth={self.depth})"
        )
