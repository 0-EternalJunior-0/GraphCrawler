# Distributed Crawling Guide ğŸš€

ĞŸÑ€Ğ¾ÑÑ‚Ğ¸Ğ¹ ÑĞ¿Ğ¾ÑÑ–Ğ± Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğ¸ Ñ€Ğ¾Ğ·Ğ¿Ğ¾Ğ´Ñ–Ğ»ĞµĞ½Ğ¸Ğ¹ ĞºÑ€Ğ°ÑƒĞ»Ñ–Ğ½Ğ³ Ñ‡ĞµÑ€ĞµĞ· YAML ĞºĞ¾Ğ½Ñ„Ñ–Ğ³ Ğ´Ğ»Ñ GraphCrawler.

---

## ğŸ“‹ Quick Start

### 1. Ğ¡Ñ‚Ğ²Ğ¾Ñ€Ğ¸Ñ‚Ğ¸ config.yaml

```yaml
broker:
  type: redis
  host: server11.example.com
  port: 6379

database:
  type: mongodb
  host: server12.example.com
  port: 27017
  database: crawler_results

crawl_task:
  urls:
    - https://example.com
  max_depth: 3
  max_pages: 1000
  extractors:
    - phones
    - emails
    - prices
```

### 2. Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğ¸ Redis (Server 11)

```bash
docker run -d -p 6379:6379 redis:latest
```

### 3. Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğ¸ MongoDB (Server 12)

```bash
docker run -d -p 27017:27017 mongo:latest
```

### 4. Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğ¸ Celery Workers (Servers 1-10)

ĞĞ° ĞºĞ¾Ğ¶Ğ½Ğ¾Ğ¼Ñƒ ÑĞµÑ€Ğ²ĞµÑ€Ñ–:
```bash
# Clone repo
git clone https://gitlab.com/demoprogrammer/web_graf.git
cd web_graf

# Install
pip install -e .

# Start worker
celery -A graph_crawler worker --loglevel=info --concurrency=4
```

### 5. Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğ¸ Coordinator (Local)

```python
from graph_crawler.distributed import EasyDistributedCrawler

crawler = EasyDistributedCrawler.from_yaml("config.yaml")
results = crawler.crawl()

# ĞÑ‚Ñ€Ğ¸Ğ¼Ğ°Ñ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸
stats = results.get_stats()
print(f"Ğ—Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾: {stats['total_nodes']} ÑÑ‚Ğ¾Ñ€Ñ–Ğ½Ğ¾Ğº")
print(f"ĞŸĞ¾ÑĞ¸Ğ»Ğ°Ğ½ÑŒ: {stats['total_edges']}")

# Extractors Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸
for node in results.nodes.values():
    phones = node.user_data.get('phones', [])
    emails = node.user_data.get('emails', [])
    prices = node.user_data.get('prices', [])
    
    if phones or emails or prices:
        print(f"\n{node.url}:")
        if phones:
            print(f"  Ğ¢ĞµĞ»ĞµÑ„Ğ¾Ğ½Ğ¸: {phones}")
        if emails:
            print(f"  Emails: {emails}")
        if prices:
            print(f"  Ğ¦Ñ–Ğ½Ğ¸: {prices}")
```

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DISTRIBUTED ARCHITECTURE                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  LOCAL (Master)                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚ Coordinator  â”‚                                               â”‚
â”‚  â”‚ (EasyDist...) â”‚                                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚         â”‚                                                        â”‚
â”‚         â”‚ 1. Push tasks                                         â”‚
â”‚         â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚  REDIS BROKER (Server 11)           â”‚                        â”‚
â”‚  â”‚  - Task queue: graph_crawler        â”‚                        â”‚
â”‚  â”‚  - Results backend                  â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚           â”‚                                                      â”‚
â”‚           â”‚ 2. Workers pull tasks                               â”‚
â”‚           â–¼                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚         CELERY WORKERS (Servers 1-10)          â”‚             â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚             â”‚
â”‚  â”‚  â”‚ Worker 1 â”‚  â”‚ Worker 2 â”‚  â”‚ Worker N â”‚     â”‚             â”‚
â”‚  â”‚  â”‚ + Driver â”‚  â”‚ + Driver â”‚  â”‚ + Driver â”‚     â”‚             â”‚
â”‚  â”‚  â”‚ +Plugins â”‚  â”‚ +Plugins â”‚  â”‚ +Plugins â”‚     â”‚             â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â”‚             â”‚
â”‚  â”‚       â”‚             â”‚             â”‚            â”‚             â”‚
â”‚  â”‚       â”‚ 3. Scan pages & extract data           â”‚             â”‚
â”‚  â”‚       â–¼             â–¼             â–¼            â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚           â”‚                                                      â”‚
â”‚           â”‚ 4. Save results                                     â”‚
â”‚           â–¼                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚  MONGODB (Server 12)                â”‚                        â”‚
â”‚  â”‚  - Collection: nodes                â”‚                        â”‚
â”‚  â”‚  - Collection: edges                â”‚                        â”‚
â”‚  â”‚  - user_data: phones, emails, etc.  â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Configuration Options

### Broker Configuration

```yaml
broker:
  type: redis          # Ğ°Ğ±Ğ¾ rabbitmq
  host: localhost      # Ğ°Ğ´Ñ€ĞµÑĞ° Ğ±Ñ€Ğ¾ĞºĞµÑ€Ğ°
  port: 6379          # Ğ¿Ğ¾Ñ€Ñ‚ Ğ±Ñ€Ğ¾ĞºĞµÑ€Ğ°
  db: 0               # Ğ½Ğ¾Ğ¼ĞµÑ€ Ğ‘Ğ” (Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ Ğ´Ğ»Ñ Redis)
  password: null      # Ğ¿Ğ°Ñ€Ğ¾Ğ»ÑŒ (Ğ¾Ğ¿Ñ†Ñ–Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)
```

### Database Configuration

```yaml
database:
  type: mongodb        # Ğ°Ğ±Ğ¾ postgresql
  host: localhost      # Ğ°Ğ´Ñ€ĞµÑĞ° Ğ‘Ğ”
  port: 27017         # Ğ¿Ğ¾Ñ€Ñ‚ Ğ‘Ğ”
  database: crawler   # Ğ½Ğ°Ğ·Ğ²Ğ° Ğ‘Ğ”
  username: null      # Ñ–Ğ¼'Ñ ĞºĞ¾Ñ€Ğ¸ÑÑ‚ÑƒĞ²Ğ°Ñ‡Ğ° (Ğ¾Ğ¿Ñ†Ñ–Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)
  password: null      # Ğ¿Ğ°Ñ€Ğ¾Ğ»ÑŒ (Ğ¾Ğ¿Ñ†Ñ–Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)
```

### Proxy Configuration (Optional)

```yaml
proxy:
  enabled: true
  type: file          # Ğ°Ğ±Ğ¾ api
  source: ./proxies.txt  # ÑˆĞ»ÑÑ… Ğ´Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ñƒ Ğ°Ğ±Ğ¾ API URL
```

### Crawl Task Configuration

```yaml
crawl_task:
  urls:
    - https://example1.com
    - https://example2.com
  max_depth: 3        # Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ° Ğ³Ğ»Ğ¸Ğ±Ğ¸Ğ½Ğ°
  max_pages: 1000     # Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ ÑÑ‚Ğ¾Ñ€Ñ–Ğ½Ğ¾Ğº
  extractors:
    - phones          # Ğ²Ğ¸Ñ‚ÑĞ³ÑƒĞ²Ğ°Ñ‚Ğ¸ Ñ‚ĞµĞ»ĞµÑ„Ğ¾Ğ½Ğ¸
    - emails          # Ğ²Ğ¸Ñ‚ÑĞ³ÑƒĞ²Ğ°Ñ‚Ğ¸ emails
    - prices          # Ğ²Ğ¸Ñ‚ÑĞ³ÑƒĞ²Ğ°Ñ‚Ğ¸ Ñ†Ñ–Ğ½Ğ¸
  custom_plugins: []  # custom Ğ¿Ğ»Ğ°Ğ³Ñ–Ğ½Ğ¸ (import paths)
```

### Workers Configuration

```yaml
workers: 10                      # ĞºÑ–Ğ»ÑŒĞºÑ–ÑÑ‚ÑŒ Ğ²Ğ¾Ñ€ĞºĞµÑ€Ñ–Ğ²
task_time_limit: 600             # Ğ»Ñ–Ğ¼Ñ–Ñ‚ Ñ‡Ğ°ÑÑƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñ– (ÑĞµĞº)
worker_prefetch_multiplier: 4    # Ğ¿Ñ€ĞµÑ„ĞµÑ‚Ñ‡ Ğ¼Ğ½Ğ¾Ğ¶Ğ½Ğ¸Ğº
```

---

## ğŸ”Œ Extractors

### Phone Extractor

Ğ’Ğ¸Ñ‚ÑĞ³ÑƒÑ” Ñ‚ĞµĞ»ĞµÑ„Ğ¾Ğ½Ğ½Ñ– Ğ½Ğ¾Ğ¼ĞµÑ€Ğ¸ Ğ· Ñ€Ñ–Ğ·Ğ½Ğ¸Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñ–Ğ²:

- **UA**: +380XXXXXXXXX, 0XXXXXXXXX, (0XX) XXX-XX-XX
- **RU**: +7XXXXXXXXXX
- **US**: +1XXXXXXXXXX, (XXX) XXX-XXXX
- **International**: +XXXXXXXXXXXX
- **tel: links**: `<a href="tel:+380...">`

```python
phones = node.user_data.get('phones', [])
# ['380501234567', '380441234567']
```

### Email Extractor

Ğ’Ğ¸Ñ‚ÑĞ³ÑƒÑ” email Ğ°Ğ´Ñ€ĞµÑĞ¸:

- **RFC 5322 compliant** regex
- **mailto: links** parsing
- **Ğ¤Ñ–Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ñ–Ñ** fake domains (example.com, test.com)
- **Deduplication** (lowercase)

```python
emails = node.user_data.get('emails', [])
# ['info@example.com', 'support@example.com']
```

### Price Extractor

Ğ’Ğ¸Ñ‚ÑĞ³ÑƒÑ” Ñ†Ñ–Ğ½Ğ¸ Ñ‚Ğ° Ğ·Ğ°Ñ€Ğ¿Ğ»Ğ°Ñ‚Ğ¸:

- **USD**: $50, $1,000, $1.5k, $1M
- **EUR**: â‚¬50, 50â‚¬, 50 EUR
- **UAH**: â‚´50, 50 Ğ³Ñ€Ğ½, 50 Ğ³Ñ€Ğ¸Ğ²ĞµĞ½ÑŒ
- **Salary ranges**: $50k - $70k, Ğ²Ñ–Ğ´ 30000 Ğ³Ñ€Ğ½

```python
prices = node.user_data.get('prices', [])
# [{'value': '$1000', 'currency': 'USD', 'original': '$1,000'}]
```

---

## ğŸ”§ Advanced Usage

### Custom Plugins

Ğ”Ğ¾Ğ´Ğ°Ğ¹Ñ‚Ğµ Ğ²Ğ»Ğ°ÑĞ½Ñ– Ğ¿Ğ»Ğ°Ğ³Ñ–Ğ½Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ñ‚ÑĞ³ÑƒĞ²Ğ°Ğ½Ğ½Ñ Ğ´Ğ°Ğ½Ğ¸Ñ…:

```python
# myapp/plugins.py
from graph_crawler.plugins.node.base import BaseNodePlugin, NodePluginType, NodePluginContext

class CustomExtractorPlugin(BaseNodePlugin):
    @property
    def name(self):
        return "CustomExtractor"
    
    @property
    def plugin_type(self):
        return NodePluginType.ON_HTML_PARSED
    
    def execute(self, context: NodePluginContext):
        # Ğ’Ğ°ÑˆĞ° Ğ»Ğ¾Ğ³Ñ–ĞºĞ° Ğ²Ğ¸Ñ‚ÑĞ³ÑƒĞ²Ğ°Ğ½Ğ½Ñ Ğ´Ğ°Ğ½Ğ¸Ñ…
        context.user_data['custom_data'] = "value"
        return context
```

Ğ£ config.yaml:

```yaml
crawl_task:
  custom_plugins:
    - "myapp.plugins.CustomExtractorPlugin"
```

### Programmatic Configuration

```python
from graph_crawler.distributed import EasyDistributedCrawler

config = {
    "broker": {
        "type": "redis",
        "host": "localhost",
        "port": 6379
    },
    "database": {
        "type": "mongodb",
        "host": "localhost",
        "port": 27017,
        "database": "test"
    },
    "crawl_task": {
        "urls": ["https://example.com"],
        "max_depth": 3,
        "extractors": ["phones", "emails"]
    }
}

crawler = EasyDistributedCrawler.from_dict(config)
results = crawler.crawl()
```

---

## METRICS Statistics

```python
stats = crawler.get_stats()

print(f"Pages crawled: {stats['pages_crawled']}")
print(f"Total nodes: {stats['total_nodes']}")
print(f"Total edges: {stats['total_edges']}")
print(f"Celery workers: {stats['celery_workers']}")
```

---

## ğŸ› Troubleshooting

### Workers Ğ½Ğµ Ğ¿Ñ–Ğ´ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ÑŒÑÑ Ğ´Ğ¾ Redis

```bash
# ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€Ğ¸Ñ‚Ğ¸ Ñ‡Ğ¸ Ğ¿Ñ€Ğ°Ñ†ÑÑ” Redis
redis-cli ping
# ĞœĞ°Ñ” Ğ¿Ğ¾Ğ²ĞµÑ€Ğ½ÑƒÑ‚Ğ¸: PONG

# ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€Ğ¸Ñ‚Ğ¸ Ñ‡Ğ¸ worker Ğ±Ğ°Ñ‡Ğ¸Ñ‚ÑŒ Redis
celery -A graph_crawler inspect active
```

### MongoDB connection failed

```bash
# ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€Ğ¸Ñ‚Ğ¸ Ñ‡Ğ¸ Ğ¿Ñ€Ğ°Ñ†ÑÑ” MongoDB
mongosh --eval "db.adminCommand('ping')"

# ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€Ğ¸Ñ‚Ğ¸ connection string
python -c "from pymongo import MongoClient; print(MongoClient('mongodb://localhost:27017/').server_info())"
```

### Tasks Ğ½Ğµ Ğ²Ğ¸ĞºĞ¾Ğ½ÑƒÑÑ‚ÑŒÑÑ

```bash
# ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€Ğ¸Ñ‚Ğ¸ Ñ‡ĞµÑ€Ğ³Ğ¸
celery -A graph_crawler inspect reserved

# ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€Ğ¸Ñ‚Ğ¸ Ğ»Ğ¾Ğ³Ğ¸ workers
celery -A graph_crawler worker --loglevel=debug
```

---

## ğŸ“š References

- [CelerySpider Source](../crawler/celery_spider.py)
- [Config Schema](config.py)
- [Extractors](../plugins/node/extractors/)
- [Example Config](../../examples/distributed/config.yaml)

---

## ğŸ¯ Best Practices

1. **Start small**: ĞŸĞ¾Ñ‡Ğ½Ñ–Ñ‚ÑŒ Ğ· 2-3 workers Ñ– Ğ·Ğ±Ñ–Ğ»ÑŒÑˆÑƒĞ¹Ñ‚Ğµ Ğ¿Ğ¾ÑÑ‚ÑƒĞ¿Ğ¾Ğ²Ğ¾
2. **Monitor workers**: Ğ’Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ¹Ñ‚Ğµ `celery flower` Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ñ–Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ñƒ
3. **Rate limiting**: Ğ”Ğ¾Ğ´Ğ°Ğ¹Ñ‚Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ¸Ğ¼ĞºĞ¸ Ğ¼Ñ–Ğ¶ Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ğ°Ğ¼Ğ¸ (`request_delay`)
4. **Error handling**: Workers Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ retry Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ°Ñ… (3 ÑĞ¿Ñ€Ğ¾Ğ±Ğ¸)
5. **Storage**: Ğ”Ğ»Ñ >100k ÑÑ‚Ğ¾Ñ€Ñ–Ğ½Ğ¾Ğº Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ¹Ñ‚Ğµ PostgreSQL Ğ·Ğ°Ğ¼Ñ–ÑÑ‚ÑŒ MongoDB

---

**Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾!** ğŸ‰

Ğ¢ĞµĞ¿ĞµÑ€ Ğ²Ğ¸ Ğ¼Ğ°Ñ”Ñ‚Ğµ Ğ¿Ğ¾Ğ²Ğ½Ñ–ÑÑ‚Ñ Ğ½Ğ°Ğ»Ğ°ÑˆÑ‚Ğ¾Ğ²Ğ°Ğ½Ñƒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ñ€Ğ¾Ğ·Ğ¿Ğ¾Ğ´Ñ–Ğ»ĞµĞ½Ğ¾Ğ³Ğ¾ ĞºÑ€Ğ°ÑƒĞ»Ñ–Ğ½Ğ³Ñƒ Ğ· Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¸Ğ¼ Ğ²Ğ¸Ñ‚ÑĞ³ÑƒĞ²Ğ°Ğ½Ğ½ÑĞ¼ Ñ‚ĞµĞ»ĞµÑ„Ğ¾Ğ½Ñ–Ğ², emails Ñ‚Ğ° Ñ†Ñ–Ğ½!
