"""Basic Example 4: API Levels v2.0

Ğ¦ĞµĞ¹ Ğ¿Ñ€Ğ¸ĞºĞ»Ğ°Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·ÑƒÑ” Ñ€Ñ–Ğ·Ğ½Ñ– Ñ€Ñ–Ğ²Ğ½Ñ– API Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸ Ğ· GraphCrawler v2.0:
- Level 1: gc.crawl() - Ğ½Ğ°Ğ¹Ğ¿Ñ€Ğ¾ÑÑ‚Ñ–ÑˆĞ¸Ğ¹ (ÑĞº requests.get)
- Level 2: gc.Crawler - reusable (ÑĞº requests.Session)
- Level 3: ApplicationContainer - Ğ´Ğ»Ñ ĞµĞºÑĞ¿ĞµÑ€Ñ‚Ñ–Ğ²

Ğ’Ğ¸ Ğ½Ğ°Ğ²Ñ‡Ğ¸Ñ‚ĞµÑÑ:
- ĞšĞ¾Ğ»Ğ¸ Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ²Ğ°Ñ‚Ğ¸ ÑĞºĞ¸Ğ¹ Ñ€Ñ–Ğ²ĞµĞ½ÑŒ API
- Extension points Ğ² ĞºĞ¾Ğ¶Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ñ–Ğ²Ğ½Ñ–
- String shortcuts Ğ´Ğ»Ñ driver/storage
- Callbacks Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ñ–Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ñƒ

Ğ¡Ğ°Ğ¹Ñ‚ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚ÑƒĞ²Ğ°Ğ½Ğ½Ñ: https://www.royalroad.com/
"""

import logging
import graph_crawler as gc

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def level_1_simple_crawl():
    """
    Level 1: gc.crawl() - ĞĞ°Ğ¹Ğ¿Ñ€Ğ¾ÑÑ‚Ñ–ÑˆĞ¸Ğ¹ ÑĞ¿Ğ¾ÑÑ–Ğ±

    Ğ¯Ğº requests.get() - Ğ¾Ğ´Ğ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ñ–Ñ!

    ĞŸĞµÑ€ĞµĞ²Ğ°Ğ³Ğ¸:
    - ĞĞ´Ğ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ñ–Ñ - Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ²Ğ¸ĞºĞ»Ğ¸ĞºĞ°Ñ‚Ğ¸
    - Ğ’ÑÑ– extension points Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ–
    - ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»Ñ–Ğ½Ğ½Ñ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸

    âŒ ĞĞ±Ğ¼ĞµĞ¶ĞµĞ½Ğ½Ñ:
    - ĞĞµ reusable
    - ĞĞ¾Ğ²Ğ¸Ğ¹ container ĞºĞ¾Ğ¶ĞµĞ½ Ñ€Ğ°Ğ·
    """
    logger.info("=" * 60)
    logger.info("Level 1: gc.crawl() - ĞĞ°Ğ¹Ğ¿Ñ€Ğ¾ÑÑ‚Ñ–ÑˆĞ¸Ğ¹")
    logger.info("=" * 60)

    # ĞŸÑ€Ğ¾ÑÑ‚Ğ¾ Ğ²Ğ¸ĞºĞ»Ğ¸ĞºĞ°Ñ”Ğ¼Ğ¾ Ñ„ÑƒĞ½ĞºÑ†Ñ–Ñ!
    graph = gc.crawl(
        url="https://www.royalroad.com/",
        max_pages=15,
        max_depth=2,
        same_domain=True
    )

    stats = graph.get_stats()
    logger.info(f"\nĞ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚:")
    logger.info(f"   ğŸ“„ Ğ¡Ñ‚Ğ¾Ñ€Ñ–Ğ½Ğ¾Ğº: {stats['total_nodes']}")
    logger.info(f"   ğŸ”— ĞŸĞ¾ÑĞ¸Ğ»Ğ°Ğ½ÑŒ: {stats['total_edges']}")
    logger.info(f"\nğŸ’¡ Ğ’Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ¹Ñ‚Ğµ ĞºĞ¾Ğ»Ğ¸: ÑˆĞ²Ğ¸Ğ´ĞºĞµ Ñ‚ĞµÑÑ‚ÑƒĞ²Ğ°Ğ½Ğ½Ñ, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ– ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¸")

    return graph


def level_1_with_extensions():
    """
    Level 1b: gc.crawl() Ğ· extension points

    ĞŸĞ¾ĞºĞ°Ğ·ÑƒÑ” Ñ‰Ğ¾ Ğ²ÑÑ– Ğ¼Ğ¾Ğ¶Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚Ñ– Ğ·Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ñ–!
    """
    logger.info("\n" + "=" * 60)
    logger.info("Level 1b: gc.crawl() Ğ· Ğ²ÑÑ–Ğ¼Ğ° extension points")
    logger.info("=" * 60)

    # Callback Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑƒ
    def on_progress(data):
        logger.info(f"   ğŸ“ˆ Progress: {data.get('progress_pct', 0)}%")

    # Ğ— ÑƒÑÑ–Ğ¼Ğ° extension points
    graph = gc.crawl(
        url="https://www.royalroad.com/",
        max_depth=2,
        max_pages=15,
        same_domain=True,

        # Driver - string shortcut!
        driver="http",  # Ğ°Ğ±Ğ¾ "async", "playwright", CustomDriver()

        # Storage - string shortcut!
        storage="memory",  # Ğ°Ğ±Ğ¾ "json", "sqlite", CustomStorage()

        # Plugins
        # plugins=[CustomPlugin()],

        # Custom Node class
        # node_class=MyNode,

        # URL rules
        # url_rules=[URLRule(...)],

        # Callbacks
        # on_progress=on_progress,
        # on_node_scanned=lambda d: print(f"Scanned: {d.get('url')}"),
        # on_error=lambda d: print(f"Error: {d}"),

        # Advanced
        request_delay=0.5,
    )

    stats = graph.get_stats()
    logger.info(f"\nĞ— extension points:")
    logger.info(f"   ğŸ“„ Ğ¡Ñ‚Ğ¾Ñ€Ñ–Ğ½Ğ¾Ğº: {stats['total_nodes']}")
    logger.info(f"\nğŸ’¡ Ğ’ÑÑ– extension points Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ– Ğ² gc.crawl()!")

    return graph


def level_2_crawler_class():
    """
    Level 2: gc.Crawler - Reusable (ÑĞº requests.Session)

    ĞŸĞµÑ€ĞµĞ²Ğ°Ğ³Ğ¸:
    - ĞœĞ¾Ğ¶Ğ½Ğ° Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ±Ğ°Ğ³Ğ°Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ñ–Ğ²
    - ĞœĞµÑ‚Ğ¾Ğ´Ğ¸ save/load
    - Default Ğ½Ğ°Ğ»Ğ°ÑˆÑ‚ÑƒĞ²Ğ°Ğ½Ğ½Ñ

    âŒ ĞĞ±Ğ¼ĞµĞ¶ĞµĞ½Ğ½Ñ:
    - ĞŸĞ¾Ñ‚Ñ€Ñ–Ğ±Ğ½Ğ¾ Ğ·Ğ°ĞºÑ€Ğ¸Ğ²Ğ°Ñ‚Ğ¸ (Ğ°Ğ±Ğ¾ with)
    """
    logger.info("\n" + "=" * 60)
    logger.info("Level 2: gc.Crawler - Reusable")
    logger.info("=" * 60)

    # Ğ¡Ñ‚Ğ²Ğ¾Ñ€ÑÑ”Ğ¼Ğ¾ crawler Ğ· default Ğ½Ğ°Ğ»Ğ°ÑˆÑ‚ÑƒĞ²Ğ°Ğ½Ğ½ÑĞ¼Ğ¸
    crawler = gc.Crawler(
        max_depth=2,
        max_pages=15,
        driver="http",
    )

    try:
        # ĞœĞ¾Ğ¶ĞµĞ¼Ğ¾ Ğ²Ğ¸ĞºĞ»Ğ¸ĞºĞ°Ñ‚Ğ¸ Ğ±Ğ°Ğ³Ğ°Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ñ–Ğ²!
        graph1 = crawler.crawl(
            url="https://www.royalroad.com/",
            timeout=60
        )

        stats1 = graph1.get_stats()
        logger.info(f"\nĞŸĞµÑ€ÑˆĞ¸Ğ¹ ĞºÑ€Ğ°ÑƒĞ»Ñ–Ğ½Ğ³:")
        logger.info(f"   ğŸ“„ Ğ¡Ñ‚Ğ¾Ñ€Ñ–Ğ½Ğ¾Ğº: {stats1['total_nodes']}")

        # ĞœĞ¾Ğ¶Ğ½Ğ° ĞºÑ€Ğ°ÑƒĞ»Ğ¸Ñ‚Ğ¸ Ñ–Ğ½ÑˆĞ¸Ğ¹ ÑĞ°Ğ¹Ñ‚!
        # graph2 = crawler.crawl("https://example.org")

        # ĞœĞ¾Ğ¶Ğ½Ğ° Ğ·Ğ±ĞµÑ€ĞµĞ³Ñ‚Ğ¸
        # crawler.save(graph1, "royalroad")

        logger.info(f"\nğŸ’¡ Ğ’Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ¹Ñ‚Ğµ ĞºĞ¾Ğ»Ğ¸: Ğ±Ğ°Ğ³Ğ°Ñ‚Ğ¾ ĞºÑ€Ğ°ÑƒĞ»Ñ–Ğ², save/load")

        return graph1

    finally:
        crawler.close()


def level_2_with_context_manager():
    """
    Level 2b: gc.Crawler Ğ· context manager (Ğ Ğ•ĞšĞĞœĞ•ĞĞ”ĞĞ’ĞĞĞ!)

    ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğµ Ğ·Ğ°ĞºÑ€Ğ¸Ñ‚Ñ‚Ñ Ñ€ĞµÑÑƒÑ€ÑÑ–Ğ².
    """
    logger.info("\n" + "=" * 60)
    logger.info("Level 2b: gc.Crawler Ğ· context manager")
    logger.info("=" * 60)

    # with Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ·Ğ°ĞºÑ€Ğ¸Ñ” Ñ€ĞµÑÑƒÑ€ÑĞ¸
    with gc.Crawler(max_depth=2, max_pages=15) as crawler:
        graph = crawler.crawl("https://www.royalroad.com/")

        stats = graph.get_stats()
        logger.info(f"\nĞ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚:")
        logger.info(f"   ğŸ“„ Ğ¡Ñ‚Ğ¾Ñ€Ñ–Ğ½Ğ¾Ğº: {stats['total_nodes']}")
        logger.info(f"\nğŸ’¡ ĞšÑ€Ğ°Ñ‰Ğµ Ğ· with - Ğ½Ğµ Ğ·Ğ°Ğ±ÑƒĞ´ĞµÑ‚Ğµ Ğ·Ğ°ĞºÑ€Ğ¸Ñ‚Ğ¸!")

        return graph


def level_3_full_container():
    """
    Level 3: ApplicationContainer - Ğ”Ğ»Ñ ĞµĞºÑĞ¿ĞµÑ€Ñ‚Ñ–Ğ²

    ĞŸĞµÑ€ĞµĞ²Ğ°Ğ³Ğ¸:
    - ĞŸĞ¾Ğ²Ğ½Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ²ÑÑ–Ğ¼Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸
    - Event bus Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ñ–Ğ¹
    - Dependency Injection

    âŒ Ğ¡ĞºĞ»Ğ°Ğ´Ğ½Ñ–ÑˆĞµ:
    - Ğ‘Ñ–Ğ»ÑŒÑˆĞµ ĞºĞ¾Ğ´Ñƒ
    - ĞŸĞ¾Ñ‚Ñ€Ñ–Ğ±Ğ½Ğ¾ Ñ€Ğ¾Ğ·ÑƒĞ¼Ñ–Ñ‚Ğ¸ Ğ°Ñ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ
    """
    logger.info("\n" + "=" * 60)
    logger.info("Level 3: ApplicationContainer - Ğ•ĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¸Ğ¹")
    logger.info("=" * 60)

    from graph_crawler.containers import ApplicationContainer
    from graph_crawler.core.configs import CrawlerConfig

    container = ApplicationContainer()

    try:
        config = CrawlerConfig(
            max_depth=2,
            max_pages=15,
            request_delay=0.5
        )
        container.config.override(config)

        # Event bus Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ñ–Ğ¹
        event_bus = container.event_bus()

        def on_node_scanned(event_name, event_data):
            logger.info(f"   ğŸ” Scanned: {event_data.get('url', 'unknown')[:40]}...")

        event_bus.subscribe('NODE_SCANNED', on_node_scanned)

        # Crawler service
        crawler_service = container.crawler_service()

        logger.info("\nğŸš€ Starting crawl with events:")
        graph = crawler_service.crawl("https://www.royalroad.com/")

        stats = graph.get_stats()
        logger.info(f"\nĞ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚:")
        logger.info(f"   ğŸ“„ Ğ¡Ñ‚Ğ¾Ñ€Ñ–Ğ½Ğ¾Ğº: {stats['total_nodes']}")
        logger.info(f"\nğŸ’¡ Ğ’Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ¹Ñ‚Ğµ ĞºĞ¾Ğ»Ğ¸: production, ÑĞºĞ»Ğ°Ğ´Ğ½Ğ° Ğ»Ğ¾Ğ³Ñ–ĞºĞ°")

        return graph

    finally:
        container.shutdown_resources()


def comparison_table():
    """ĞŸĞ¾Ñ€Ñ–Ğ²Ğ½ÑĞ»ÑŒĞ½Ğ° Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ Ğ²ÑÑ–Ñ… Ñ€Ñ–Ğ²Ğ½Ñ–Ğ² API v2.0"""
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“Š API LEVELS COMPARISON v2.0")
    logger.info("=" * 80)

    comparison = """
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature        â”‚ L1: gc.crawl()   â”‚ L2: gc.Crawler   â”‚ L3: Container    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ°        â”‚ â­â­â­â­â­           â”‚ â­â­â­â­             â”‚ â­â­               â”‚
â”‚ Ğ“Ğ½ÑƒÑ‡ĞºÑ–ÑÑ‚ÑŒ       â”‚ â­â­â­â­            â”‚ â­â­â­â­             â”‚ â­â­â­â­â­           â”‚
â”‚ Reusable       â”‚ âŒ               â”‚ Ñ‚Ğ°Ğº            â”‚ Ñ‚Ğ°Ğº            â”‚
â”‚ ĞšĞ¾Ğ´ (Ñ€ÑĞ´ĞºÑ–Ğ²)   â”‚ 1-5             â”‚ 5-15            â”‚ 15-30           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ĞšĞ¾Ğ»Ğ¸           â”‚ Ğ¨Ğ²Ğ¸Ğ´ĞºÑ– ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¸  â”‚ Ğ‘Ğ°Ğ³Ğ°Ñ‚Ğ¾ ĞºÑ€Ğ°ÑƒĞ»Ñ–Ğ²   â”‚ Production      â”‚
â”‚ Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚.      â”‚ Ğ¢ĞµÑÑ‚ÑƒĞ²Ğ°Ğ½Ğ½Ñ       â”‚ Save/Load       â”‚ Ğ¡ĞºĞ»Ğ°Ğ´Ğ½Ğ° Ğ»Ğ¾Ğ³Ñ–ĞºĞ°  â”‚
â”‚                â”‚ ĞŸÑ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ¸        â”‚                  â”‚ Events          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¯ EXTENSION POINTS (Ğ’Ğ¡Ğ† Ğ·Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ñ– Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğ¼Ñƒ API!):

   driver="http"           # Ğ°Ğ±Ğ¾ "async", "playwright", CustomDriver()
   storage="memory"        # Ğ°Ğ±Ğ¾ "json", "sqlite", CustomStorage()
   plugins=[Plugin()]      # Ğ²Ğ°ÑˆÑ– Ğ¿Ğ»Ğ°Ğ³Ñ–Ğ½Ğ¸
   node_class=MyNode       # ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ğ¸Ğ¹ Node
   url_rules=[URLRule()]   # Ñ„Ñ–Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ñ–Ñ URL
   on_progress=callback    # callbacks
"""

    logger.info(comparison)


def best_practices():
    """ĞĞ°Ğ¹ĞºÑ€Ğ°Ñ‰Ñ– Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ¸ v2.0"""
    logger.info("\n" + "=" * 80)
    logger.info("BEST PRACTICES v2.0")
    logger.info("=" * 80)

    practices = """
ğŸ“Œ Level 1 (gc.crawl):
   DO:
      graph = gc.crawl("https://example.com", max_pages=100)
      graph = gc.crawl(..., driver="playwright")  # String shortcut!
      graph = gc.crawl(..., on_progress=callback)  # Callbacks!
   
   âŒ DON'T:
      # ĞĞµ Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ¹Ñ‚Ğµ Ğ² Ñ†Ğ¸ĞºĞ»Ğ°Ñ… (Ğ½Ğ¾Ğ²Ğ¸Ğ¹ container ĞºĞ¾Ğ¶ĞµĞ½ Ñ€Ğ°Ğ·)

ğŸ“Œ Level 2 (gc.Crawler):
   DO:
      with gc.Crawler(max_depth=5) as crawler:  # Context manager!
          graph1 = crawler.crawl("https://site1.com")
          graph2 = crawler.crawl("https://site2.com")
          crawler.save(graph1, "site1")
   
   âŒ DON'T:
      crawler = gc.Crawler()
      graph = crawler.crawl(...)  # Ğ—Ğ°Ğ±ÑƒĞ»Ğ¸ crawler.close()!

ğŸ“Œ Level 3 (Container):
   DO:
      # Ğ’Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ¹Ñ‚Ğµ ĞºĞ¾Ğ»Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€Ñ–Ğ±ĞµĞ½ Ğ¿Ğ¾Ğ²Ğ½Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ
      # ĞŸÑ–Ğ´Ğ¿Ğ¸ÑÑƒĞ¹Ñ‚ĞµÑÑŒ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ñ–Ñ— Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ñ–Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ñƒ
      container.shutdown_resources()  # Ğ—Ğ°Ğ²Ğ¶Ğ´Ğ¸!
   
   âŒ DON'T:
      # ĞĞµ Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ¹Ñ‚Ğµ ÑĞºÑ‰Ğ¾ gc.crawl() Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ½ÑŒĞ¾
"""

    logger.info(practices)


if __name__ == "__main__":
    print("\nğŸš€ GraphCrawler v2.0 API Levels Examples\n")

    try:
        # ĞŸĞ¾ĞºĞ°Ğ·ÑƒÑ”Ğ¼Ğ¾ Ğ²ÑÑ– Ñ€Ñ–Ğ²Ğ½Ñ–
        graph1 = level_1_simple_crawl()
        graph2 = level_1_with_extensions()
        graph3 = level_2_crawler_class()
        graph4 = level_2_with_context_manager()
        graph5 = level_3_full_container()

        # ĞŸĞ¾Ñ€Ñ–Ğ²Ğ½ÑĞ»ÑŒĞ½Ğ° Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ
        comparison_table()

        # Best practices
        best_practices()

        print("\n" + "=" * 80)
        print("All API levels demonstrated successfully!")
        print("=" * 80)

    except Exception as e:
        logger.error(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        raise
