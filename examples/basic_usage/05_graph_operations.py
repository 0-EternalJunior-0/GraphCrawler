"""Basic Example 5: Graph Operations v2.0

–¶–µ–π –ø—Ä–∏–∫–ª–∞–¥ –ø–æ–∫–∞–∑—É—î –±–∞–∑–æ–≤—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó –∑ –≥—Ä–∞—Ñ–∞–º–∏ –∑ –Ω–æ–≤–∏–º API v2.0:
- –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≥—Ä–∞—Ñ—É —á–µ—Ä–µ–∑ gc.crawl()
- –û–±'—î–¥–Ω–∞–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–≤ (union)
- –†—ñ–∑–Ω–∏—Ü—è –º—ñ–∂ –≥—Ä–∞—Ñ–∞–º–∏ (difference)
- –ü–æ—à—É–∫ –≤—É–∑–ª—ñ–≤
- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥—Ä–∞—Ñ—É

–°–∞–π—Ç –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è: https://www.royalroad.com/
"""

import logging
import graph_crawler as gc

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def example_1_simple_crawl():
    """–ü—Ä–∏–∫–ª–∞–¥ 1: –ù–∞–π–ø—Ä–æ—Å—Ç—ñ—à–∏–π –∫—Ä–∞—É–ª—ñ–Ω–≥ –∑ gc.crawl()"""
    logger.info("=" * 60)
    logger.info("Example 1: Simple Crawl with gc.crawl()")
    logger.info("=" * 60)

    # –û–¥–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è!
    graph = gc.crawl(
        "https://www.royalroad.com/",
        max_pages=10,
        max_depth=2
    )

    stats = graph.get_stats()
    logger.info(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç:")
    logger.info(f"   üìÑ –í—É–∑–ª—ñ–≤: {stats['total_nodes']}")
    logger.info(f"   üîó –†–µ–±–µ—Ä: {stats['total_edges']}")

    return graph


def example_2_manual_graph_building():
    """–ü—Ä–∏–∫–ª–∞–¥ 2: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≥—Ä–∞—Ñ—É –≤—Ä—É—á–Ω—É"""
    logger.info("\n" + "=" * 60)
    logger.info("Example 2: Manual Graph Building")
    logger.info("=" * 60)

    # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ–π –≥—Ä–∞—Ñ
    graph = gc.Graph()

    # –°—Ç–≤–æ—Ä—é—î–º–æ –≤—É–∑–ª–∏ –≤—Ä—É—á–Ω—É
    node1 = gc.Node(
        url="https://www.royalroad.com/fictions/best-rated",
        title="Best Rated Fictions",
        depth=0
    )

    node2 = gc.Node(
        url="https://www.royalroad.com/fictions/best-rated?page=2",
        title="Best Rated Page 2",
        depth=1
    )

    node3 = gc.Node(
        url="https://www.royalroad.com/fictions/best-rated?page=3",
        title="Best Rated Page 3",
        depth=1
    )

    # –î–æ–¥–∞—î–º–æ –≤—É–∑–ª–∏
    graph.add_node(node1)
    graph.add_node(node2)
    graph.add_node(node3)

    # –°—Ç–≤–æ—Ä—é—î–º–æ —Ä–µ–±—Ä–∞
    edge1 = gc.Edge(
        source_node_id=node1.url,
        target_node_id=node2.url
    )
    edge2 = gc.Edge(
        source_node_id=node1.url,
        target_node_id=node3.url
    )

    graph.add_edge(edge1)
    graph.add_edge(edge2)

    stats = graph.get_stats()
    logger.info(f"\n–°—Ç–≤–æ—Ä–µ–Ω–æ –≥—Ä–∞—Ñ –≤—Ä—É—á–Ω—É:")
    logger.info(f"   üìÑ –í—É–∑–ª—ñ–≤: {stats['total_nodes']}")
    logger.info(f"   üîó –†–µ–±–µ—Ä: {stats['total_edges']}")

    return graph


def example_3_graph_union():
    """–ü—Ä–∏–∫–ª–∞–¥ 3: –û–±'—î–¥–Ω–∞–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–≤"""
    logger.info("\n" + "=" * 60)
    logger.info("Example 3: Graph Union")
    logger.info("=" * 60)

    # –ö—Ä–∞—É–ª–∏–º–æ –¥–≤–∞ —Ä—ñ–∑–Ω—ñ —Ä–æ–∑–¥—ñ–ª–∏
    graph1 = gc.crawl(
        url="https://www.royalroad.com/fictions/best-rated",
        max_pages=10,
        max_depth=1
    )

    graph2 = gc.crawl(
        url="https://www.royalroad.com/fictions/trending",
        max_pages=10,
        max_depth=1
    )

    stats1 = graph1.get_stats()
    stats2 = graph2.get_stats()

    logger.info(f"\nüìä –î–æ –æ–±'—î–¥–Ω–∞–Ω–Ω—è:")
    logger.info(f"   Graph 1: {stats1['total_nodes']} –≤—É–∑–ª—ñ–≤")
    logger.info(f"   Graph 2: {stats2['total_nodes']} –≤—É–∑–ª—ñ–≤")

    # –û–±'—î–¥–Ω—É—î–º–æ
    from graph_crawler.core.graph_operations import GraphOperations

    combined = GraphOperations.union(graph1, graph2)
    stats_combined = combined.get_stats()

    logger.info(f"\n–ü—ñ—Å–ª—è –æ–±'—î–¥–Ω–∞–Ω–Ω—è:")
    logger.info(f"   Combined: {stats_combined['total_nodes']} –≤—É–∑–ª—ñ–≤")
    logger.info(f"   (–º–æ–∂–µ –±—É—Ç–∏ –º–µ–Ω—à–µ —è–∫—â–æ –±—É–ª–∏ –¥—É–±–ª—ñ–∫–∞—Ç–∏)")

    return combined


def example_4_graph_difference():
    """–ü—Ä–∏–∫–ª–∞–¥ 4: –†—ñ–∑–Ω–∏—Ü—è –º—ñ–∂ –≥—Ä–∞—Ñ–∞–º–∏ (–∑–º—ñ–Ω–∏ –Ω–∞ —Å–∞–π—Ç—ñ)"""
    logger.info("\n" + "=" * 60)
    logger.info("Example 4: Graph Difference (–ó–º—ñ–Ω–∏)")
    logger.info("=" * 60)

    logger.info("\nüïê –ü–µ—Ä—à–∏–π —Å–∫–∞–Ω...")
    graph_old = gc.crawl(
        url="https://www.royalroad.com/",
        max_pages=10,
        max_depth=2
    )

    logger.info("\nüïë –î—Ä—É–≥–∏–π —Å–∫–∞–Ω...")
    graph_new = gc.crawl(
        url="https://www.royalroad.com/",
        max_pages=15,  # –ë—ñ–ª—å—à–µ —Å—Ç–æ—Ä—ñ–Ω–æ–∫
        max_depth=2
    )

    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Ä—ñ–∑–Ω–∏—Ü—é
    from graph_crawler.core.graph_operations import GraphOperations

    diff = GraphOperations.difference(graph_new, graph_old)

    stats_old = graph_old.get_stats()
    stats_new = graph_new.get_stats()
    stats_diff = diff.get_stats()

    logger.info(f"\nüìä –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è:")
    logger.info(f"   –°—Ç–∞—Ä–∏–π: {stats_old['total_nodes']} –≤—É–∑–ª—ñ–≤")
    logger.info(f"   –ù–æ–≤–∏–π: {stats_new['total_nodes']} –≤—É–∑–ª—ñ–≤")
    logger.info(f"   –ù–æ–≤—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏: {stats_diff['total_nodes']} –≤—É–∑–ª—ñ–≤")

    # –ü–æ–∫–∞–∑—É—î–º–æ –Ω–æ–≤—ñ URL
    if stats_diff['total_nodes'] > 0:
        logger.info("\n–ù–æ–≤—ñ URL (–ø–µ—Ä—à—ñ 5):")
        for i, url in enumerate(list(diff.nodes.keys())[:5]):
            logger.info(f"   {i+1}. {url[:60]}...")

    return diff


def example_5_node_search():
    """–ü—Ä–∏–∫–ª–∞–¥ 5: –ü–æ—à—É–∫ –≤—É–∑–ª—ñ–≤"""
    logger.info("\n" + "=" * 60)
    logger.info("Example 5: Node Search")
    logger.info("=" * 60)

    graph = gc.crawl(
        url="https://www.royalroad.com/",
        max_pages=25,
        max_depth=2
    )

    # –ü–æ—à—É–∫ –∑–∞ URL
    logger.info("\nüîç –ü–æ—à—É–∫ –∑–∞ URL pattern '/fiction/'...")
    fiction_nodes = [node for url, node in graph.nodes.items() if '/fiction/' in url]
    logger.info(f"   –ó–Ω–∞–π–¥–µ–Ω–æ {len(fiction_nodes)} fiction —Å—Ç–æ—Ä—ñ–Ω–æ–∫")

    # –ü–æ—à—É–∫ –∑–∞ –≥–ª–∏–±–∏–Ω–æ—é
    logger.info("\nüîç –ü–æ—à—É–∫ –∑–∞ –≥–ª–∏–±–∏–Ω–æ—é (depth=1)...")
    depth_1 = [node for url, node in graph.nodes.items()
               if hasattr(node, 'depth') and node.depth == 1]
    logger.info(f"   –ó–Ω–∞–π–¥–µ–Ω–æ {len(depth_1)} –≤—É–∑–ª—ñ–≤ –Ω–∞ –≥–ª–∏–±–∏–Ω—ñ 1")

    # –ü–æ—à—É–∫ –≤—ñ–¥—Å–∫–∞–Ω–æ–≤–∞–Ω–∏—Ö
    logger.info("\nüîç –ü–æ—à—É–∫ –≤—ñ–¥—Å–∫–∞–Ω–æ–≤–∞–Ω–∏—Ö...")
    scanned = [node for url, node in graph.nodes.items()
               if hasattr(node, 'scanned') and node.scanned]
    logger.info(f"   –ó–Ω–∞–π–¥–µ–Ω–æ {len(scanned)} –≤—ñ–¥—Å–∫–∞–Ω–æ–≤–∞–Ω–∏—Ö")

    return graph


def example_6_graph_statistics():
    """–ü—Ä–∏–∫–ª–∞–¥ 6: –î–µ—Ç–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥—Ä–∞—Ñ—É"""
    logger.info("\n" + "=" * 60)
    logger.info("Example 6: Detailed Graph Statistics")
    logger.info("=" * 60)

    graph = gc.crawl(
        url="https://www.royalroad.com/",
        max_pages=25,
        max_depth=2
    )

    stats = graph.get_stats()

    logger.info("\nüìä –î–µ—Ç–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    logger.info(f"\nüåê –í—É–∑–ª–∏ (Nodes):")
    logger.info(f"   –í—Å—å–æ–≥–æ: {stats['total_nodes']}")
    logger.info(f"   –í—ñ–¥—Å–∫–∞–Ω–æ–≤–∞–Ω—ñ: {stats['scanned_nodes']}")
    logger.info(f"   –û—á—ñ–∫—É—é—Ç—å: {stats['pending_nodes']}")

    logger.info(f"\nüîó –†–µ–±—Ä–∞ (Edges):")
    logger.info(f"   –í—Å—å–æ–≥–æ: {stats['total_edges']}")

    # –†–æ–∑–ø–æ–¥—ñ–ª –∑–∞ –≥–ª–∏–±–∏–Ω–æ—é
    logger.info(f"\nüìà –†–æ–∑–ø–æ–¥—ñ–ª –∑–∞ –≥–ª–∏–±–∏–Ω–æ—é:")
    depth_dist = {}
    for url, node in graph.nodes.items():
        if hasattr(node, 'depth'):
            depth = node.depth
            depth_dist[depth] = depth_dist.get(depth, 0) + 1

    for depth in sorted(depth_dist.keys()):
        logger.info(f"   Depth {depth}: {depth_dist[depth]} –≤—É–∑–ª—ñ–≤")

    # –î–æ–º–µ–Ω–∏
    logger.info(f"\nüåç –î–æ–º–µ–Ω–∏:")
    from urllib.parse import urlparse
    domains = {}
    for url, node in graph.nodes.items():
        domain = urlparse(url).netloc
        domains[domain] = domains.get(domain, 0) + 1

    for domain, count in sorted(domains.items(), key=lambda x: x[1], reverse=True)[:5]:
        logger.info(f"   {domain}: {count} —Å—Ç–æ—Ä—ñ–Ω–æ–∫")

    return graph


if __name__ == "__main__":
    print("\nüöÄ GraphCrawler v2.0 Graph Operations Examples\n")

    try:
        graph1 = example_1_simple_crawl()
        graph2 = example_2_manual_graph_building()
        graph3 = example_3_graph_union()
        graph4 = example_4_graph_difference()
        graph5 = example_5_node_search()
        graph6 = example_6_graph_statistics()

        print("\n" + "=" * 60)
        print("All graph operations examples completed!")
        print("=" * 60)

    except Exception as e:
        logger.error(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        raise
