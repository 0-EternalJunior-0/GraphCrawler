"""Basic Example 3: URL Patterns and Filtering

–¶–µ–π –ø—Ä–∏–∫–ª–∞–¥ –ø–æ–∫–∞–∑—É—î —è–∫ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –∑ URL patterns - —â–æ —Å–∫–∞–Ω—É–≤–∞—Ç–∏, —â–æ –ø—Ä–æ–ø—É—Å–∫–∞—Ç–∏.
–í–∏ –Ω–∞–≤—á–∏—Ç–µ—Å—è:
- URLRule –¥–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó
- Regex patterns –¥–ª—è URL
- –ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–∞–º —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è
- –Ü–≥–Ω–æ—Ä—É–≤–∞–Ω–Ω—é —Ñ–∞–π–ª—ñ–≤ (PDF, images, —Ç–æ—â–æ)

–°–∞–π—Ç –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è: https://www.royalroad.com/
"""

from graph_crawler import crawl
from graph_crawler.core.models import URLRule
import logging
import re

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def example_1_basic_url_rules():
    """–ü—Ä–∏–∫–ª–∞–¥ 1: –ë–∞–∑–æ–≤—ñ URL rules"""
    logger.info("=" * 60)
    logger.info("Example 1: Basic URL Rules")
    logger.info("=" * 60)

    # URLRule –¥–æ–∑–≤–æ–ª—è—î –∫–æ–Ω—Ç—Ä–æ–ª—é–≤–∞—Ç–∏ —è–∫—ñ URL —Å–∫–∞–Ω—É–≤–∞—Ç–∏
    url_rules = [
        # –°–∫–∞–Ω—É–≤–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ fiction
        URLRule(
            pattern=r".*/fiction/.*",  # Regex pattern
            should_scan=True,
            priority=10  # –í–∏—Å–æÔøΩÔøΩ–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç
        ),
        # –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ —Ñ–æ—Ä—É–º
        URLRule(
            pattern=r".*/forums/.*",
            should_scan=False
        ),
    ]

    graph = crawl(
        url="https://www.royalroad.com/",
        max_pages=30,
        max_depth=2,
        url_rules=url_rules
    )

    # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
    fiction_pages = 0
    forum_pages = 0
    other_pages = 0

    for node in graph.nodes.values():
        if hasattr(node, 'url'):
            if '/fiction/' in node.url:
                fiction_pages += 1
            elif '/forums/' in node.url:
                forum_pages += 1
            else:
                other_pages += 1

    logger.info(f"\nüìä Results:")
    logger.info(f"   üìö Fiction pages: {fiction_pages}")
    logger.info(f"   üí¨ Forum pages: {forum_pages} (should be 0)")
    logger.info(f"   üìÑ Other pages: {other_pages}")

    return graph


def example_2_ignore_file_types():
    """–ü—Ä–∏–∫–ª–∞–¥ 2: –Ü–≥–Ω–æ—Ä—É–≤–∞–Ω–Ω—è —Ç–∏–ø—ñ–≤ —Ñ–∞–π–ª—ñ–≤"""
    logger.info("\n" + "=" * 60)
    logger.info("Example 2: Ignore File Types")
    logger.info("=" * 60)

    # –ß–∞—Å—Ç–æ —Ç—Ä–µ–±–∞ —ñ–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ —Ñ–∞–π–ª–∏ (PDF, images, videos, —Ç–æ—â–æ)
    url_rules = [
        # –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ PDF —Ñ–∞–π–ª–∏
        URLRule(pattern=r".*\.pdf$", should_scan=False),
        # –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
        URLRule(pattern=r".*\.(jpg|jpeg|png|gif|svg|webp)$", should_scan=False),
        # –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ –≤—ñ–¥–µ–æ
        URLRule(pattern=r".*\.(mp4|avi|mov|wmv)$", should_scan=False),
        # –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ –∞—Ä—Ö—ñ–≤–∏
        URLRule(pattern=r".*\.(zip|rar|7z|tar|gz)$", should_scan=False),
    ]

    graph = crawl(
        url="https://www.royalroad.com/",
        max_pages=25,
        max_depth=2,
        url_rules=url_rules
    )

    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —â–æ –Ω–µ –±—É–ª–æ —Ñ–∞–π–ª—ñ–≤
    file_extensions = set()
    for node in graph.nodes.values():
        if hasattr(node, 'url'):
            url = node.url.lower()
            if '.' in url.split('/')[-1]:  # —î —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è
                ext = url.split('.')[-1].split('?')[0]  # –≤–∏—Ç—è–≥–∞—î–º–æ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è
                file_extensions.add(ext)

    logger.info(f"\nFound file extensions: {sorted(file_extensions)}")
    logger.info("üí° Note: No images/PDFs should be in the list")

    return graph


def example_3_priority_based_crawling():
    """–ü—Ä–∏–∫–ª–∞–¥ 3: –ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–Ω–µ —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è"""
    logger.info("\n" + "=" * 60)
    logger.info("Example 3: Priority-based Crawling")
    logger.info("=" * 60)

    # –ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç –≤–∏–∑–Ω–∞—á–∞—î –ø–æ—Ä—è–¥–æ–∫ —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è
    url_rules = [
        # –ù–∞–π–≤–∏—â–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç - fiction —Å—Ç–æ—Ä—ñ–Ω–∫–∏
        URLRule(
            pattern=r".*/fiction/\d+/.*",  # /fiction/123/chapter-name
            priority=100,
            should_scan=True
        ),
        # –°–µ—Ä–µ–¥–Ω—ñ–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç - –æ–≥–ª—è–¥–∏
        URLRule(
            pattern=r".*/reviews/.*",
            priority=50,
            should_scan=True
        ),
        # –ù–∏–∑—å–∫–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç - –≤—Å–µ —ñ–Ω—à–µ
        URLRule(
            pattern=r".*",
            priority=1,
            should_scan=True
        ),
    ]

    graph = crawl(
        url="https://www.royalroad.com/",
        max_pages=30,
        max_depth=2,
        url_rules=url_rules
    )

    stats = graph.get_stats()
    logger.info(f"\nScanned {stats['scanned_nodes']} pages")
    logger.info("üìå High-priority pages were scanned first")

    return graph


def example_4_complex_patterns():
    """–ü—Ä–∏–∫–ª–∞–¥ 4: –°–∫–ª–∞–¥–Ω—ñ URL patterns"""
    logger.info("\n" + "=" * 60)
    logger.info("Example 4: Complex URL Patterns")
    logger.info("=" * 60)

    url_rules = [
        # –¢—ñ–ª—å–∫–∏ –∞–Ω–≥–ª—ñ–π—Å—å–∫—ñ fiction
        URLRule(
            pattern=r".*/fiction/\d+/[a-z0-9-]+$",
            should_scan=True,
            priority=90
        ),
        # –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (query strings)
        URLRule(
            pattern=r".*\?.*",  # –º—ñ—Å—Ç–∏—Ç—å ?
            should_scan=False
        ),
        # –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ anchor links
        URLRule(
            pattern=r".*#.*",  # –º—ñ—Å—Ç–∏—Ç—å #
            should_scan=False
        ),
        # –¢—ñ–ª—å–∫–∏ HTTPS (–±–µ–∑–ø–µ–∫–∞)
        URLRule(
            pattern=r"^https://.*",
            should_scan=True
        ),
        URLRule(
            pattern=r"^http://.*",  # HTTP –±–µ–∑ S
            should_scan=False
        ),
    ]

    graph = crawl(
        url="https://www.royalroad.com/",
        max_pages=20,
        max_depth=2,
        url_rules=url_rules
    )

    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —â–æ –≤—Å—ñ URL –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—é—Ç—å –ø—Ä–∞–≤–∏–ª–∞–º
    https_count = 0
    with_params = 0

    for node in graph.nodes.values():
        if hasattr(node, 'url'):
            if node.url.startswith('https://'):
                https_count += 1
            if '?' in node.url:
                with_params += 1

    logger.info(f"\nHTTPS URLs: {https_count}")
    logger.info(f"‚ùå URLs with params: {with_params} (should be 0)")

    return graph


def example_5_whitelist_blacklist():
    """–ü—Ä–∏–∫–ª–∞–¥ 5: Whitelist —Ç–∞ Blacklist –ø—ñ–¥—Ö—ñ–¥"""
    logger.info("\n" + "=" * 60)
    logger.info("Example 5: Whitelist/Blacklist Approach")
    logger.info("=" * 60)

    # Whitelist –ø—ñ–¥—Ö—ñ–¥ - –¥–æ–∑–≤–æ–ª—è—î–º–æ —Ç—ñ–ª—å–∫–∏ –ø–µ–≤–Ω—ñ URL
    whitelist_rules = [
        URLRule(pattern=r".*/fiction/.*", should_scan=True, priority=10),
        URLRule(pattern=r".*/author/.*", should_scan=True, priority=5),
        URLRule(pattern=r".*", should_scan=False, priority=0),  # –í—Å–µ —ñ–Ω—à–µ - –Ω—ñ
    ]

    graph_whitelist = crawl(
        url="https://www.royalroad.com/",
        max_pages=20,
        max_depth=2,
        url_rules=whitelist_rules
    )

    logger.info(f"\nüéØ Whitelist approach:")
    logger.info(f"   Scanned: {len(graph_whitelist.nodes)} pages")

    # Blacklist –ø—ñ–¥—Ö—ñ–¥ - –±–ª–æ–∫—É—î–º–æ –ø–µ–≤–Ω—ñ URL, —Ä–µ—à—Ç–∞ –¥–æ–∑–≤–æ–ª–µ–Ω–∞
    blacklist_rules = [
        URLRule(pattern=r".*/forums/.*", should_scan=False),
        URLRule(pattern=r".*/user/.*", should_scan=False),
        URLRule(pattern=r".*/private/.*", should_scan=False),
        # –í—Å–µ —ñ–Ω—à–µ –¥–æ–∑–≤–æ–ª–µ–Ω–æ (–Ω–µ–º–∞—î –∑–∞–≥–∞–ª—å–Ω–æ–≥–æ –ø—Ä–∞–≤–∏–ª–∞ –∑ should_scan=True)
    ]

    graph_blacklist = crawl(
        url="https://www.royalroad.com/",
        max_pages=20,
        max_depth=2,
        url_rules=blacklist_rules
    )

    logger.info(f"\nüö´ Blacklist approach:")
    logger.info(f"   Scanned: {len(graph_blacklist.nodes)} pages")

    logger.info("\nüí° When to use:")
    logger.info("   - Whitelist: –∫–æ–ª–∏ –∑–Ω–∞—î—Ç–µ —Ç–æ—á–Ω–æ —â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ")
    logger.info("   - Blacklist: –∫–æ–ª–∏ –∑–Ω–∞—î—Ç–µ —â–æ –ù–ï –ø–æ—Ç—Ä—ñ–±–Ω–æ")

    return graph_whitelist, graph_blacklist


if __name__ == "__main__":
    print("\nüöÄ Starting URL Pattern Examples\n")

    try:
        example_1_basic_url_rules()
        example_2_ignore_file_types()
        example_3_priority_based_crawling()
        example_4_complex_patterns()
        example_5_whitelist_blacklist()

        print("\n" + "=" * 60)
        print("All URL pattern examples completed!")
        print("=" * 60)

    except Exception as e:
        logger.error(f"‚ùå Error: {e}")
        raise
