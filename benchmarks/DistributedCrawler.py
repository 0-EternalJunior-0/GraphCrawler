import redis
import sys
import time
from functools import wraps
import graph_crawler as gc
from graph_crawler import AsyncDriver

# ==============================
# –î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –≤–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è —á–∞—Å—É
# ==============================
def measure_time(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        times = []
        for i in range(1):
            start = time.time()
            result = func(*args, **kwargs)
            end = time.time()
            duration = end - start
            times.append(duration)
            print(f"{func.__name__} | –∑–∞–ø—É—Å–∫ {i+1}: {duration:.6f} —Å–µ–∫—É–Ω–¥/ len(graph) = {len(result)}")

        avg = sum(times) / len(times)
        print(f"‚û° –°–µ—Ä–µ–¥–Ω—ñ–π —á–∞—Å –¥–ª—è {func.__name__}: {avg:.6f} —Å–µ–∫—É–Ω–¥\n")
        return result
    return wrapper


# ==============================
# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ Redis —Ç–∞ workers
# ==============================
def check_redis_and_workers(host: str, port: int, retries=5, delay=2):
    """–ü–µ—Ä–µ–≤—ñ—Ä—è—î –¥–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å Redis —Ç–∞ Celery workers."""
    print("=" * 50)
    print("üîç –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è...")
    print("=" * 50)
    
    # 1. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ Redis
    for i in range(retries):
        try:
            r = redis.Redis(host=host, port=port)
            if r.ping():
                print(f"‚úÖ Redis –¥–æ—Å—Ç—É–ø–Ω–∏–π –Ω–∞ {host}:{port}")
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–µ—Ä–≥–∏
                queues = r.keys("*celery*") or []
                if queues:
                    print(f"   –ó–Ω–∞–π–¥–µ–Ω—ñ —á–µ—Ä–≥–∏: {[q.decode() for q in queues[:5]]}")
                break
        except redis.ConnectionError:
            print(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –ø—ñ–¥–∫–ª—é—á–∏—Ç–∏—Å—è –¥–æ Redis {host}:{port}, —Å–ø—Ä–æ–±–∞ {i+1}/{retries}")
            time.sleep(delay)
    else:
        print("üö® Redis –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é.")
        sys.exit(1)
    
    # 2. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ Celery workers
    print("\nüîç –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ Celery workers...")
    try:
        from graph_crawler.infrastructure.messaging.celery_unified import celery
        
        # –ù–∞–ª–∞—à—Ç–æ–≤—É—î–º–æ broker URL
        broker_url = f"redis://{host}:{port}/0"
        celery.conf.update(broker_url=broker_url, result_backend=f"redis://{host}:{port}/1")
        
        inspect = celery.control.inspect(timeout=5)
        ping_result = inspect.ping()
        
        if ping_result:
            worker_count = len(ping_result)
            worker_names = list(ping_result.keys())
            print(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {worker_count} worker(s): {', '.join(worker_names)}")
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∞–∫—Ç–∏–≤–Ω—ñ –∑–∞–¥–∞—á—ñ
            active = inspect.active() or {}
            active_count = sum(len(tasks) for tasks in active.values())
            print(f"   –ê–∫—Ç–∏–≤–Ω–∏—Ö –∑–∞–¥–∞—á: {active_count}")
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–µ—Ä–≥–∏ —è–∫—ñ —Å–ª—É—Ö–∞—é—Ç—å workers
            queues = inspect.active_queues() or {}
            for worker, worker_queues in queues.items():
                queue_names = [q.get('name', 'unknown') for q in worker_queues]
                print(f"   {worker}: —á–µ—Ä–≥–∏ {queue_names}")
        else:
            print("‚ö†Ô∏è  Workers –Ω–µ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—é—Ç—å! –ü–µ—Ä–µ–∫–æ–Ω–∞–π—Ç–µ—Å—å —â–æ worker –∑–∞–ø—É—â–µ–Ω–∏–π:")
            print("   docker compose up -d")
            print("   docker compose logs -f worker")
    except Exception as e:
        print(f"‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ workers: {e}")
    
    print("=" * 50)


# ==============================
# –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–ª—è distributed
# ==============================
# –í–ê–ñ–õ–ò–í–û: host —Ç–∞ port –ø–æ–≤–∏–Ω–Ω—ñ –±—É—Ç–∏ –¥–æ—Å—Ç—É–ø–Ω—ñ —è–∫ –∑ –∫–ª—ñ—î–Ω—Ç–∞ —Ç–∞–∫ —ñ –∑ worker
# –Ø–∫—â–æ worker –≤ Docker - –≤—ñ–Ω –ø—ñ–¥–∫–ª—é—á–∞—î—Ç—å—Å—è —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä—ñ—à–Ω—é –º–µ—Ä–µ–∂—É (redis:6379)
# –Ø–∫—â–æ –∫–ª—ñ—î–Ω—Ç –∑–∑–æ–≤–Ω—ñ - –≤—ñ–Ω –ø—ñ–¥–∫–ª—é—á–∞—î—Ç—å—Å—è —á–µ—Ä–µ–∑ –ø—É–±–ª—ñ—á–Ω–∏–π IP:port
config = {
    "broker": {
        "type": "redis",
        "host": "45.159.248.146",  # –ü—É–±–ª—ñ—á–Ω–∏–π IP –≤–∞—à–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞
        "port": 6579               # –ü–æ—Ä—Ç Redis (–ø—Ä–æ–±—Ä–æ—à–µ–Ω–∏–π –∑ Docker)
    },
    "database": {"type": "memory"},
    
    # ========== –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø –ß–ï–†–ï–ó –ü–£–õ–¨–¢ ==========
    "batch_size": 12,              # URLs –≤ –æ–¥–Ω—ñ–π –∑–∞–¥–∞—á—ñ (–±—ñ–ª—å—à–µ = —à–≤–∏–¥—à–µ, –∞–ª–µ –±—ñ–ª—å—à–µ RAM)
    "worker_prefetch_multiplier": 64  # –°–∫—ñ–ª—å–∫–∏ –∑–∞–¥–∞—á –±—Ä–∞—Ç–∏ –Ω–∞–ø–µ—Ä–µ–¥
}

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ Redis —Ç–∞ workers
check_redis_and_workers(config["broker"]["host"], config["broker"]["port"])

batch_async = 30

# ==============================
# –§—É–Ω–∫—Ü—ñ—è –∫—Ä–∞—É–ª—ñ–Ω–≥—É
# ==============================
@measure_time
def distributed_crawl():
    """–¢–µ—Å—Ç distributed –∫—Ä–∞—É–ª—ñ–Ω–≥—É."""
    print("\nüöÄ –ó–∞–ø—É—Å–∫ distributed crawl...")
    
    graph = gc.crawl(
        "https://netpeak.net/",
        max_depth=2,
        max_pages=1 + batch_async * 10,
        wrapper=config,
        driver=gc.AsyncDriver,
        edge_strategy=gc.EdgeCreationStrategy.NEW_ONLY,
        timeout=120  # 2 —Ö–≤–∏–ª–∏–Ω–∏ timeout
    )
    return graph


if __name__ == "__main__":
    try:
        result = distributed_crawl()
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç:")
        print(f"   –í—É–∑–ª—ñ–≤: {len(result.nodes)}")
        print(f"   –†–µ–±–µ—Ä: {len(result.edges)}")
        print(f"\n‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ!")
    except Exception as e:
        print(f"\n‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")
        import traceback
        traceback.print_exc()