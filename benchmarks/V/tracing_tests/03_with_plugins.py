"""
============================================================
–¢–ï–°–¢ 3: –ö–†–ê–£–õ–Ü–ù–ì –ó –ü–õ–ê–ì–Ü–ù–ê–ú–ò
============================================================

–ü–æ–∫–∞–∑—É—î:
1. –Ø–∫ —Å—Ç–≤–æ—Ä–∏—Ç–∏ –≤–ª–∞—Å–Ω–∏–π –ø–ª–∞–≥—ñ–Ω
2. –¢–∏–ø–∏ –ø–ª–∞–≥—ñ–Ω—ñ–≤: ON_BEFORE_SCAN, ON_AFTER_SCAN, ON_AFTER_PARSE
3. –Ø–∫ –ø–ª–∞–≥—ñ–Ω–∏ –º–æ–¥–∏—Ñ—ñ–∫—É—é—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
4. –õ–∞–Ω—Ü—é–∂–æ–∫ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –ø–ª–∞–≥—ñ–Ω—ñ–≤
"""

import sys
import os
import asyncio
import logging
from datetime import datetime
from typing import Optional, Any, List
from pydantic import Field

# –®–ª—è—Ö –¥–æ –ø—Ä–æ–µ–∫—Ç—É
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
sys.path.insert(0, PROJECT_ROOT)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-7s | %(message)s',
    datefmt='%H:%M:%S'
)

logger = logging.getLogger(__name__)


def print_section(title: str):
    print("\n" + "=" * 80)
    print(f" {title}")
    print("=" * 80)


def print_step(step: int, description: str):
    print(f"\n{'‚îÄ' * 60}")
    print(f"  –ö–†–û–ö {step}: {description}")
    print(f"{'‚îÄ' * 60}")


async def test_with_plugins():
    """
    –¢–µ—Å—Ç –∑ –∫–∞—Å—Ç–æ–º–Ω–∏–º–∏ –ø–ª–∞–≥—ñ–Ω–∞–º–∏.
    """
    import graph_crawler as gc
    from graph_crawler import AsyncDriver
    from graph_crawler.plugins.node import BaseNodePlugin, NodePluginType
    from graph_crawler.core.models import EdgeCreationStrategy
    
    print_section("–¢–ï–°–¢ 3: –ö–†–ê–£–õ–Ü–ù–ì –ó –ü–õ–ê–ì–Ü–ù–ê–ú–ò")
    
    # ============================================================
    print_step(1, "–¢–ò–ü–ò –ü–õ–ê–ì–Ü–ù–Ü–í")
    # ============================================================
    
    print("""
    GraphCrawler –ø—ñ–¥—Ç—Ä–∏–º—É—î –¥–µ–∫—ñ–ª—å–∫–∞ —Ç–∏–ø—ñ–≤ –ø–ª–∞–≥—ñ–Ω—ñ–≤:
    
    NodePluginType:
    - ON_BEFORE_SCAN   - –ü–µ—Ä–µ–¥ —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è (–º–æ–∂–Ω–∞ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–∏ URL)
    - ON_AFTER_SCAN    - –ü—ñ—Å–ª—è —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è (–º–æ–∂–Ω–∞ —Ñ—ñ–ª—å—Ç—Ä—É–≤–∞—Ç–∏ links)
    - ON_AFTER_PARSE   - –ü—ñ—Å–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É (–º–æ–∂–Ω–∞ –º–æ–¥–∏—Ñ—ñ–∫—É–≤–∞—Ç–∏ node)
    
    EnginePlugin:
    - AntiBotStealthPlugin - –û–±—Ö—ñ–¥ anti-bot –∑–∞—Ö–∏—Å—Ç—É
    - CaptchaSolverPlugin  - –†–æ–∑–≤'—è–∑–∞–Ω–Ω—è captcha
    
    DriverPlugin:
    - StealthPlugin, CloudflarePlugin, etc.
    """)
    
    # ============================================================
    print_step(2, "–°–¢–í–û–†–ï–ù–ù–Ø –ö–ê–°–¢–û–ú–ù–ò–• –ü–õ–ê–ì–Ü–ù–Ü–í")
    # ============================================================
    
    class LoggingPlugin(BaseNodePlugin):
        """
        –ü–ª–∞–≥—ñ–Ω –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è –∫–æ–∂–Ω–æ–≥–æ –∫—Ä–æ–∫—É.
        –í–∏–∫–æ–Ω—É—î—Ç—å—Å—è ON_AFTER_SCAN.
        """
        
        @property
        def name(self) -> str:
            return "LoggingPlugin"
        
        @property
        def plugin_type(self) -> NodePluginType:
            return NodePluginType.ON_AFTER_SCAN
        
        def execute(self, context):
            print(f"\n  üîå LoggingPlugin.execute()")
            print(f"     URL: {context.url}")
            print(f"     Links found: {len(context.extracted_links)}")
            print(f"     Has HTML: {context.html_tree is not None}")
            return context
    
    class LinkFilterPlugin(BaseNodePlugin):
        """
        –ü–ª–∞–≥—ñ–Ω –¥–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –ø–æ—Å–∏–ª–∞–Ω—å.
        –í–∏–¥–∞–ª—è—î –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –∑–æ–≤–Ω—ñ—à–Ω—ñ –¥–æ–º–µ–Ω–∏.
        """
        
        def __init__(self, config: dict = None):
            super().__init__(config or {})
            self.allowed_patterns = config.get('allowed_patterns', []) if config else []
        
        @property
        def name(self) -> str:
            return "LinkFilterPlugin"
        
        @property
        def plugin_type(self) -> NodePluginType:
            return NodePluginType.ON_AFTER_SCAN
        
        def execute(self, context):
            original_count = len(context.extracted_links)
            
            # –§—ñ–ª—å—Ç—Ä—É—î–º–æ links
            if self.allowed_patterns:
                filtered = [
                    link for link in context.extracted_links
                    if any(pattern in link for pattern in self.allowed_patterns)
                ]
                context.extracted_links = filtered
            
            print(f"\n  üîå LinkFilterPlugin.execute()")
            print(f"     Original links: {original_count}")
            print(f"     Filtered links: {len(context.extracted_links)}")
            
            return context
    
    class MetadataPlugin(BaseNodePlugin):
        """
        –ü–ª–∞–≥—ñ–Ω –¥–ª—è –∑–±–æ—Ä—É –º–µ—Ç–∞–¥–∞–Ω–∏—Ö.
        –ó–±–µ—Ä—ñ–≥–∞—î custom –¥–∞–Ω—ñ –≤ user_data.
        """
        
        @property
        def name(self) -> str:
            return "MetadataPlugin"
        
        @property
        def plugin_type(self) -> NodePluginType:
            return NodePluginType.ON_AFTER_SCAN
        
        def execute(self, context):
            print(f"\n  üîå MetadataPlugin.execute()")
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –º–µ—Ç–∞–¥–∞–Ω—ñ –≤ user_data
            context.user_data['processed_at'] = datetime.now().isoformat()
            context.user_data['plugin_version'] = '1.0'
            
            # –í–∏—Ç—è–≥—É—î–º–æ –º–µ—Ç–∞-—Ç–µ–≥–∏ —è–∫—â–æ —î HTML
            if context.html_tree is not None:
                # BeautifulSoup API
                if hasattr(context.html_tree, 'find_all'):
                    meta_tags = context.html_tree.find_all('meta', attrs={'content': True})
                    context.user_data['meta_count'] = len(meta_tags)
                    print(f"     Found {len(meta_tags)} meta tags")
                # Fallback –¥–ª—è lxml
                elif hasattr(context.html_tree, 'xpath'):
                    meta_tags = context.html_tree.xpath('//meta/@content')
                    context.user_data['meta_count'] = len(meta_tags)
                    print(f"     Found {len(meta_tags)} meta tags")
            
            print(f"     Added metadata to user_data")
            return context
    
    print("\n  –°—Ç–≤–æ—Ä–µ–Ω–æ 3 –∫–∞—Å—Ç–æ–º–Ω—ñ –ø–ª–∞–≥—ñ–Ω–∏:")
    print("  1. LoggingPlugin (ON_AFTER_SCAN) - –ª–æ–≥—É–≤–∞–Ω–Ω—è")
    print("  2. LinkFilterPlugin (ON_AFTER_SCAN) - —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –ø–æ—Å–∏–ª–∞–Ω—å")
    print("  3. MetadataPlugin (ON_AFTER_PARSE) - –∑–±—ñ—Ä –º–µ—Ç–∞–¥–∞–Ω–∏—Ö")
    
    # ============================================================
    print_step(3, "–Ü–ù–Ü–¶–Ü–ê–õ–Ü–ó–ê–¶–Ü–Ø –ü–õ–ê–ì–Ü–ù–Ü–í")
    # ============================================================
    
    plugins = [
        LoggingPlugin(config={}),
        LinkFilterPlugin(config={'allowed_patterns': ['httpbin']}),
        MetadataPlugin(config={}),
    ]
    
    print(f"\n  –°–ø–∏—Å–æ–∫ –ø–ª–∞–≥—ñ–Ω—ñ–≤:")
    for i, plugin in enumerate(plugins, 1):
        print(f"    {i}. {type(plugin).__name__} ({plugin.plugin_type.value})")
    
    # ============================================================
    print_step(4, "–ó–ê–ü–£–°–ö –ö–†–ê–£–õ–Ü–ù–ì–£")
    # ============================================================
    
    print("\n  >>> graph = await gc.crawl(")
    print("  ...     'https://httpbin.org/html',")
    print("  ...     max_depth=1,")
    print("  ...     max_pages=3,")
    print("  ...     plugins=[LoggingPlugin(), LinkFilterPlugin(), MetadataPlugin()]")
    print("  ... )")
    
    start_time = datetime.now()
    
    graph = await gc.crawl(
        "https://httpbin.org/html",
        max_depth=1,
        max_pages=3,
        plugins=plugins,
        driver=AsyncDriver,
        edge_strategy=EdgeCreationStrategy.NEW_ONLY,
    )
    
    duration = (datetime.now() - start_time).total_seconds()
    
    # ============================================================
    print_step(5, "–†–ï–ó–£–õ–¨–¢–ê–¢–ò")
    # ============================================================
    
    print(f"\n  –ö—Ä–∞—É–ª—ñ–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {duration:.2f} —Å–µ–∫—É–Ω–¥!")
    print(f"  –ó–Ω–∞–π–¥–µ–Ω–æ –Ω–æ–¥: {len(graph.nodes)}")
    
    print("\n  –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ user_data (–≤—ñ–¥ MetadataPlugin):")
    for node_id, node in graph.nodes.items():
        print(f"\n  Node: {node.url}")
        if node.user_data:
            for key, value in node.user_data.items():
                print(f"    user_data['{key}'] = {value}")
        else:
            print("    user_data: (empty)")
    
    # ============================================================
    print_step(6, "–ü–û–†–Ø–î–û–ö –í–ò–ö–û–ù–ê–ù–ù–Ø –ü–õ–ê–ì–Ü–ù–Ü–í")
    # ============================================================
    
    print("""
    –ü–æ—Ä—è–¥–æ–∫ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –ø–ª–∞–≥—ñ–Ω—ñ–≤ –≤ NodeScanner:
    
    NodeScanner.scan_node(node):
    ‚îÇ
    ‚îú‚îÄ‚îÄ 1. ON_BEFORE_SCAN plugins
    ‚îÇ       ‚îî‚îÄ‚îÄ –ú–æ–∂—É—Ç—å –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–∏ —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è (return skip=True)
    ‚îÇ
    ‚îú‚îÄ‚îÄ 2. driver.fetch(url)
    ‚îÇ       ‚îî‚îÄ‚îÄ HTTP –∑–∞–ø–∏—Ç –¥–æ —Å—Ç–æ—Ä—ñ–Ω–∫–∏
    ‚îÇ
    ‚îú‚îÄ‚îÄ 3. parser.parse(html)
    ‚îÇ       ‚îî‚îÄ‚îÄ –ü–∞—Ä—Å–∏–Ω–≥ HTML (lxml/BeautifulSoup)
    ‚îÇ
    ‚îú‚îÄ‚îÄ 4. ON_AFTER_SCAN plugins   ‚Üê LoggingPlugin, LinkFilterPlugin
    ‚îÇ       ‚îî‚îÄ‚îÄ –ú–æ–∂—É—Ç—å —Ñ—ñ–ª—å—Ç—Ä—É–≤–∞—Ç–∏ extracted_links
    ‚îÇ
    ‚îî‚îÄ‚îÄ 5. ON_AFTER_PARSE plugins  ‚Üê MetadataPlugin
            ‚îî‚îÄ‚îÄ –ú–æ–∂—É—Ç—å –º–æ–¥–∏—Ñ—ñ–∫—É–≤–∞—Ç–∏ node, user_data
    
    –ü–æ—Ç—ñ–º:
    ‚îî‚îÄ‚îÄ node._update_from_context(context)
        ‚îî‚îÄ‚îÄ –ó–∞—Å—Ç–æ—Å–æ–≤—É—î –∑–º—ñ–Ω–∏ –¥–æ Node
    """)
    
    print_section("–¢–ï–°–¢ 3 –ó–ê–í–ï–†–®–ï–ù–û")
    return graph


if __name__ == "__main__":
    print("\n" + "*" * 80)
    print("  GRAPHCRAWLER v3.0 - TRACING TEST 03: PLUGINS")
    print("*" * 80)
    
    graph = asyncio.run(test_with_plugins())
    
    print("\n‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ!")
