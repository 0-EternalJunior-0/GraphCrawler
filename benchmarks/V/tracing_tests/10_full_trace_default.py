"""
============================================================
–¢–†–ê–°–£–í–ê–ù–ù–Ø 10: –ü–û–í–ù–ò–ô –í–ò–ö–õ–ò–ö - –î–ï–§–û–õ–¢–ù–ê –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Ø
============================================================

–¶–µ–π —Ñ–∞–π–ª –ø–æ–∫–∞–∑—É—î –ü–û–í–ù–ò–ô –ª–∞–Ω—Ü—é–∂–æ–∫ –≤–∏–∫–ª–∏–∫—ñ–≤ –¥–ª—è –¥–µ—Ñ–æ–ª—Ç–Ω–æ–≥–æ –∫—Ä–∞—É–ª—ñ–Ω–≥—É:
- –ó–≤—ñ–¥–∫–∏ –±–µ—Ä—É—Ç—å—Å—è –¥–∞–Ω—ñ
- –Ø–∫ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—é—Ç—å—Å—è –∫–ª–∞—Å–∏
- –Ø–∫ —Å—Ç–≤–æ—Ä—é—é—Ç—å—Å—è –Ω–æ–¥–∏
- –Ø–∫ –∑–Ω–∞—Ö–æ–¥—è—Ç—å—Å—è –Ω–æ–≤—ñ URL
- –Ø–∫ –ø—Ä–∞—Ü—é—î –≥—Ä–∞—Ñ

–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:
    python 10_full_trace_default.py
"""

import sys
import os
import asyncio
import logging
from datetime import datetime
from typing import Optional, List, Any

# –®–ª—è—Ö –¥–æ –ø—Ä–æ–µ–∫—Ç—É
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
sys.path.insert(0, PROJECT_ROOT)

# –î–µ—Ç–∞–ª—å–Ω–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s | %(levelname)-7s | %(name)-50s | %(message)s',
    datefmt='%H:%M:%S'
)

# –í–∏–º–∏–∫–∞—î–º–æ –∑–∞–π–≤—ñ –ª–æ–≥–∏
for logger_name in ['urllib3', 'asyncio', 'aiohttp', 'charset_normalizer', 'httpx']:
    logging.getLogger(logger_name).setLevel(logging.WARNING)

logger = logging.getLogger(__name__)


def print_header(title: str):
    print("\n" + "=" * 100)
    print(f"  {title}")
    print("=" * 100)


def print_section(title: str):
    print(f"\n{'‚îÄ' * 80}")
    print(f"  üìå {title}")
    print(f"{'‚îÄ' * 80}")


def print_code(code: str):
    """–î—Ä—É–∫—É—î –∫–æ–¥ –∑ –ø—ñ–¥—Å–≤—ñ—Ç–∫–æ—é."""
    print(f"\n  >>> {code}")


def print_trace(source: str, action: str, details: str = ""):
    """–î—Ä—É–∫—É—î —Ç—Ä–∞—Å—É–≤–∞–Ω–Ω—è –≤–∏–∫–ª–∏–∫—É."""
    print(f"  üìç [{source}] {action}")
    if details:
        print(f"      ‚îî‚îÄ {details}")


async def trace_default_crawl():
    """
    –î–µ—Ç–∞–ª—å–Ω–µ —Ç—Ä–∞—Å—É–≤–∞–Ω–Ω—è –¥–µ—Ñ–æ–ª—Ç–Ω–æ–≥–æ –≤–∏–∫–ª–∏–∫—É gc.crawl().
    """
    print_header("–¢–†–ê–°–£–í–ê–ù–ù–Ø: –î–ï–§–û–õ–¢–ù–ò–ô –í–ò–ö–õ–ò–ö gc.crawl()")
    
    # ============================================================
    print_section("–ï–¢–ê–ü 1: –Ü–ú–ü–û–†–¢ –ë–Ü–ë–õ–Ü–û–¢–ï–ö–ò")
    # ============================================================
    
    print_code("import graph_crawler as gc")
    print("""
    –©–æ –≤—ñ–¥–±—É–≤–∞—î—Ç—å—Å—è –ø—Ä–∏ —ñ–º–ø–æ—Ä—Ç—ñ:
    
    1. Python –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î graph_crawler/__init__.py
    2. __init__.py —ñ–º–ø–æ—Ä—Ç—É—î:
       - crawl –∑ api/simple.py (–≥–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è)
       - Node, Edge, Graph –∑ core/
       - AsyncDriver, PlaywrightDriver –∑ drivers/
       - URLRule –∑ core/models.py
    """)
    
    import graph_crawler as gc
    print(f"\n  ‚úÖ –Ü–º–ø–æ—Ä—Ç —É—Å–ø—ñ—à–Ω–∏–π")
    print(f"     –í–µ—Ä—Å—ñ—è: {gc.__version__}")
    print(f"     –®–ª—è—Ö: {gc.__file__}")
    
    # ============================================================
    print_section("–ï–¢–ê–ü 2: –í–ò–ö–õ–ò–ö gc.crawl() - API ENTRY POINT")
    # ============================================================
    
    print_code("graph = await gc.crawl('https://httpbin.org/html', max_depth=1, max_pages=2)")
    print("""
    gc.crawl() –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –≤ graph_crawler/api/simple.py
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ —â–æ –ø–µ—Ä–µ–¥–∞—é—Ç—å—Å—è:
    - url: 'https://httpbin.org/html'
    - max_depth: 1
    - max_pages: 2
    - driver: None (–±—É–¥–µ —Å—Ç–≤–æ—Ä–µ–Ω–æ AsyncDriver)
    - plugins: None (–±—É–¥–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ –¥–µ—Ñ–æ–ª—Ç–Ω—ñ)
    """)
    
    # ============================================================
    print_section("–ï–¢–ê–ü 3: –í–°–ï–†–ï–î–ò–ù–Ü crawl() - –°–¢–í–û–†–ï–ù–ù–Ø –ö–û–ù–¢–ï–ô–ù–ï–†–ê")
    # ============================================================
    
    print("""
    –í api/simple.py::crawl():
    
    1. from graph_crawler.containers import ApplicationContainer
    2. container = ApplicationContainer()
    
    ApplicationContainer (DI –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä) —Å—Ç–≤–æ—Ä—é—î:
    ‚îú‚îÄ‚îÄ CoreContainer
    ‚îÇ   ‚îú‚îÄ‚îÄ EventBus - —Å–∏—Å—Ç–µ–º–∞ –ø–æ–¥—ñ–π
    ‚îÇ   ‚îî‚îÄ‚îÄ Configs - –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
    ‚îú‚îÄ‚îÄ DriverContainer  
    ‚îÇ   ‚îú‚îÄ‚îÄ http_driver - —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∏–π requests
    ‚îÇ   ‚îú‚îÄ‚îÄ async_driver - aiohttp (DEFAULT)
    ‚îÇ   ‚îî‚îÄ‚îÄ playwright_driver - –¥–ª—è JS —Å—Ç–æ—Ä—ñ–Ω–æ–∫
    ‚îú‚îÄ‚îÄ StorageContainer
    ‚îÇ   ‚îú‚îÄ‚îÄ memory_storage - –≤ RAM (DEFAULT)
    ‚îÇ   ‚îú‚îÄ‚îÄ json_storage - –≤ JSON —Ñ–∞–π–ª
    ‚îÇ   ‚îî‚îÄ‚îÄ sqlite_storage - –≤ SQLite
    ‚îú‚îÄ‚îÄ FilterContainer
    ‚îÇ   ‚îú‚îÄ‚îÄ domain_filter - —Ñ—ñ–ª—å—Ç—Ä –¥–æ–º–µ–Ω—ñ–≤
    ‚îÇ   ‚îî‚îÄ‚îÄ path_filter - —Ñ—ñ–ª—å—Ç—Ä —à–ª—è—Ö—ñ–≤
    ‚îî‚îÄ‚îÄ CrawlerContainer
        ‚îú‚îÄ‚îÄ scheduler - —á–µ—Ä–≥–∞ URL
        ‚îú‚îÄ‚îÄ scanner - —Å–∫–∞–Ω–µ—Ä —Å—Ç–æ—Ä—ñ–Ω–æ–∫
        ‚îî‚îÄ‚îÄ processor - –æ–±—Ä–æ–±–Ω–∏–∫ –ø–æ—Å–∏–ª–∞–Ω—å
    """)
    
    from graph_crawler.containers import ApplicationContainer
    container = ApplicationContainer()
    print(f"  ‚úÖ ApplicationContainer —Å—Ç–≤–æ—Ä–µ–Ω–æ: {container}")
    
    # ============================================================
    print_section("–ï–¢–ê–ü 4: –°–¢–í–û–†–ï–ù–ù–Ø CRAWLER CONFIG")
    # ============================================================
    
    print("""
    CrawlerConfig —Å—Ç–≤–æ—Ä—é—î—Ç—å—Å—è –∑ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤:
    
    config = CrawlerConfig(
        url='https://httpbin.org/html',
        max_depth=1,
        max_pages=2,
        allowed_domains=['*'],  # default
        url_rules=[],
        node_plugins=[],
        custom_node_class=None,  # –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –±–∞–∑–æ–≤–∏–π Node
        edge_strategy='all'
    )
    """)
    
    from graph_crawler.core.configs import CrawlerConfig, DriverConfig
    driver_cfg = DriverConfig(request_delay=0.5)
    config = CrawlerConfig(
        url='https://httpbin.org/html',
        max_depth=1,
        max_pages=2,
        driver=driver_cfg,
    )
    print(f"  ‚úÖ CrawlerConfig: max_depth={config.max_depth}, max_pages={config.max_pages}")
    
    # ============================================================
    print_section("–ï–¢–ê–ü 5: –°–¢–í–û–†–ï–ù–ù–Ø DRIVER")
    # ============================================================
    
    print("""
    Driver —Å—Ç–≤–æ—Ä—é—î—Ç—å—Å—è —á–µ—Ä–µ–∑ DriverFactory:
    
    from graph_crawler.factories.driver_factory import create_driver
    driver = create_driver(AsyncDriver, {})
    
    AsyncDriver (graph_crawler/drivers/async_http/driver.py):
    - –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î aiohttp –¥–ª—è HTTP –∑–∞–ø–∏—Ç—ñ–≤
    - –ü—ñ–¥—Ç—Ä–∏–º—É—î batch fetching (–ø–∞—Ä–∞–ª–µ–ª—å–Ω—ñ –∑–∞–ø–∏—Ç–∏)
    - max_concurrent_requests=24 –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
    """)
    
    from graph_crawler.drivers import AsyncDriver
    from graph_crawler.factories.driver_factory import create_driver
    driver = create_driver(AsyncDriver, {})
    print(f"  ‚úÖ Driver —Å—Ç–≤–æ—Ä–µ–Ω–æ: {type(driver).__name__}")
    
    # ============================================================
    print_section("–ï–¢–ê–ü 6: –°–¢–í–û–†–ï–ù–ù–Ø CLIENT")
    # ============================================================
    
    print("""
    GraphCrawlerClient - –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä –∫—Ä–∞—É–ª—ñ–Ω–≥—É:
    
    client = GraphCrawlerClient(
        driver=driver,
        storage=MemoryStorage(),
        event_bus=EventBus(),
        repository=GraphRepository()
    )
    
    –í—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω–æ—Å—Ç—ñ:
    - –°—Ç–≤–æ—Ä–µ–Ω–Ω—è GraphSpider
    - –ö–æ–æ—Ä–¥–∏–Ω–∞—Ü—ñ—è –∫—Ä–∞—É–ª—ñ–Ω–≥—É
    - –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≥—Ä–∞—Ñ—É
    """)
    
    from graph_crawler.client.client import GraphCrawlerClient
    event_bus = container.core.event_bus()
    storage_instance = container.storage.memory_storage()
    repository = container.repository()
    
    client = GraphCrawlerClient(
        driver=driver,
        storage=storage_instance,
        event_bus=event_bus,
        repository=repository,
    )
    print(f"  ‚úÖ Client —Å—Ç–≤–æ—Ä–µ–Ω–æ: {client}")
    
    # ============================================================
    print_section("–ï–¢–ê–ü 7: –í–ò–ö–õ–ò–ö client.crawl() ‚Üí –°–¢–í–û–†–ï–ù–ù–Ø SPIDER")
    # ============================================================
    
    print("""
    –í—Å–µ—Ä–µ–¥–∏–Ω—ñ client.crawl():
    
    1. –°—Ç–≤–æ—Ä—é—î—Ç—å—Å—è CrawlerConfig
    2. –°—Ç–≤–æ—Ä—é—î—Ç—å—Å—è GraphSpider
    3. Spider.crawl() –∑–∞–ø—É—Å–∫–∞—î—Ç—å—Å—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
    
    GraphSpider (graph_crawler/crawler/spider.py) –º—ñ—Å—Ç–∏—Ç—å:
    ‚îú‚îÄ‚îÄ graph: Graph - –≥—Ä–∞—Ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    ‚îú‚îÄ‚îÄ scheduler: CrawlScheduler - —á–µ—Ä–≥–∞ URL –∑ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–∞–º–∏
    ‚îú‚îÄ‚îÄ domain_filter: DomainFilter - —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –ø–æ –¥–æ–º–µ–Ω—É
    ‚îú‚îÄ‚îÄ path_filter: PathFilter - —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –ø–æ —à–ª—è—Ö—É
    ‚îú‚îÄ‚îÄ scanner: NodeScanner - —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–æ–∫
    ‚îú‚îÄ‚îÄ processor: LinkProcessor - –æ–±—Ä–æ–±–∫–∞ –ø–æ—Å–∏–ª–∞–Ω—å
    ‚îî‚îÄ‚îÄ coordinator: CrawlCoordinator - –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü—ñ—è –ø—Ä–æ—Ü–µ—Å—É
    """)
    
    # ============================================================
    print_section("–ï–¢–ê–ü 8: –ó–ê–ü–£–°–ö –ö–†–ê–£–õ–Ü–ù–ì–£")
    # ============================================================
    
    print("""
    Spider.crawl() –≤–∏–∫–æ–Ω—É—î:
    
    1. –°—Ç–≤–æ—Ä—é—î root_node (—Å—Ç–∞—Ä—Ç–æ–≤–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞)
       node = Node(url='https://httpbin.org/html', depth=0)
    
    2. –î–æ–¥–∞—î –≤ –≥—Ä–∞—Ñ:
       graph.add_node(root_node)
    
    3. –î–æ–¥–∞—î –≤ scheduler:
       scheduler.add_node(root_node)
    
    4. –î–µ–ª–µ–≥—É—î –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä—É:
       await coordinator.coordinate()
    """)
    
    print("\n  üöÄ –ó–∞–ø—É—Å–∫ –∫—Ä–∞—É–ª—ñ–Ω–≥—É...\n")
    
    start_time = datetime.now()
    graph = await client.crawl(
        url='https://httpbin.org/html',
        max_depth=1,
        max_pages=2,
    )
    duration = (datetime.now() - start_time).total_seconds()
    
    # ============================================================
    print_section("–ï–¢–ê–ü 9: –ö–û–û–†–î–ò–ù–ê–¢–û–† - –ì–û–õ–û–í–ù–ò–ô –¶–ò–ö–õ")
    # ============================================================
    
    print("""
    CrawlCoordinator.coordinate() –≤–∏–∫–æ–Ω—É—î –≥–æ–ª–æ–≤–Ω–∏–π —Ü–∏–∫–ª:
    
    while not scheduler.is_empty() and pages_crawled < max_pages:
        ‚îÇ
        ‚îú‚îÄ‚îÄ 1. –û—Ç—Ä–∏–º–∞—Ç–∏ –Ω–∞—Å—Ç—É–ø–Ω—É –Ω–æ–¥—É:
        ‚îÇ      node = scheduler.get_next()  # heapq –∑–∞ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–æ–º
        ‚îÇ
        ‚îú‚îÄ‚îÄ 2. –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –≥–ª–∏–±–∏–Ω—É:
        ‚îÇ      if node.depth > max_depth: skip
        ‚îÇ
        ‚îú‚îÄ‚îÄ 3. –°–∫–∞–Ω—É–≤–∞—Ç–∏ —Å—Ç–æ—Ä—ñ–Ω–∫—É:
        ‚îÇ      links = await scanner.scan_node(node)
        ‚îÇ      ‚îÇ
        ‚îÇ      ‚îú‚îÄ‚îÄ HTTP –∑–∞–ø–∏—Ç —á–µ—Ä–µ–∑ driver.fetch(url)
        ‚îÇ      ‚îú‚îÄ‚îÄ –ü–∞—Ä—Å–∏–Ω–≥ HTML (BeautifulSoup)
        ‚îÇ      ‚îú‚îÄ‚îÄ node.process_html(html)
        ‚îÇ      ‚îÇ   ‚îú‚îÄ‚îÄ –í–∏—Ç—è–≥—É—î metadata (title, h1, description)
        ‚îÇ      ‚îÇ   ‚îú‚îÄ‚îÄ –í–∏—Ç—è–≥—É—î –ø–æ—Å–∏–ª–∞–Ω–Ω—è <a href>
        ‚îÇ      ‚îÇ   ‚îú‚îÄ‚îÄ –í–∏–∫–æ–Ω—É—î –ø–ª–∞–≥—ñ–Ω–∏
        ‚îÇ      ‚îÇ   ‚îî‚îÄ‚îÄ –û—á–∏—â—É—î HTML –∑ –ø–∞–º'—è—Ç—ñ
        ‚îÇ      ‚îî‚îÄ‚îÄ –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ –ø–æ—Å–∏–ª–∞–Ω—å
        ‚îÇ
        ‚îî‚îÄ‚îÄ 4. –û–±—Ä–æ–±–∏—Ç–∏ –ø–æ—Å–∏–ª–∞–Ω–Ω—è:
               processor.process_links(node, links)
               ‚îÇ
               ‚îú‚îÄ‚îÄ –î–ª—è –∫–æ–∂–Ω–æ–≥–æ link:
               ‚îÇ   ‚îú‚îÄ‚îÄ –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è URL
               ‚îÇ   ‚îú‚îÄ‚îÄ domain_filter.should_scan(url) ‚Üí bool
               ‚îÇ   ‚îú‚îÄ‚îÄ path_filter.should_scan(url) ‚Üí bool
               ‚îÇ   ‚îú‚îÄ‚îÄ –°—Ç–≤–æ—Ä–µ–Ω–Ω—è child_node
               ‚îÇ   ‚îú‚îÄ‚îÄ graph.add_node(child_node)
               ‚îÇ   ‚îú‚îÄ‚îÄ graph.add_edge(parent, child)
               ‚îÇ   ‚îî‚îÄ‚îÄ scheduler.add_node(child_node)
               ‚îî‚îÄ‚îÄ –ü–æ–≤—Ç–æ—Ä–∏—Ç–∏ —Ü–∏–∫–ª
    """)
    
    # ============================================================
    print_section("–ï–¢–ê–ü 10: –†–ï–ó–£–õ–¨–¢–ê–¢–ò")
    # ============================================================
    
    print(f"\n  ‚è±Ô∏è –ß–∞—Å –≤–∏–∫–æ–Ω–∞–Ω–Ω—è: {duration:.2f} —Å–µ–∫—É–Ω–¥")
    print(f"  üìä –ó–Ω–∞–π–¥–µ–Ω–æ –Ω–æ–¥: {len(graph.nodes)}")
    print(f"  üîó –ó–Ω–∞–π–¥–µ–Ω–æ edges: {len(graph.edges)}")
    
    print("\n  üìã –î–µ—Ç–∞–ª—ñ –Ω–æ–¥:")
    for node_id, node in graph.nodes.items():
        print(f"\n      Node: {node.url}")
        print(f"      ‚îú‚îÄ‚îÄ depth: {node.depth}")
        print(f"      ‚îú‚îÄ‚îÄ scanned: {node.scanned}")
        print(f"      ‚îú‚îÄ‚îÄ should_scan: {node.should_scan}")
        print(f"      ‚îú‚îÄ‚îÄ lifecycle: {node.lifecycle_stage.value}")
        if node.metadata:
            print(f"      ‚îî‚îÄ‚îÄ metadata:")
            for key, value in node.metadata.items():
                val_str = str(value)[:50] + '...' if len(str(value)) > 50 else str(value)
                print(f"          ‚îú‚îÄ‚îÄ {key}: {val_str}")
    
    # –ó–∞–∫—Ä–∏–≤–∞—î–º–æ driver
    await driver.close()
    
    print_header("–¢–†–ê–°–£–í–ê–ù–ù–Ø –ó–ê–í–ï–†–®–ï–ù–û")
    return graph


if __name__ == "__main__":
    print("\n" + "*" * 100)
    print("  GRAPHCRAWLER v3.0 - –ü–û–í–ù–ï –¢–†–ê–°–£–í–ê–ù–ù–Ø –î–ï–§–û–õ–¢–ù–û–ì–û –í–ò–ö–õ–ò–ö–£")
    print("*" * 100)
    
    graph = asyncio.run(trace_default_crawl())
    print("\n‚úÖ –¢—Ä–∞—Å—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ!")
