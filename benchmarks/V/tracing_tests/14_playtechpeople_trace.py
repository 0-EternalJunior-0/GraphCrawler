"""
============================================================
Ğ¢Ğ ĞĞ¡Ğ£Ğ’ĞĞĞĞ¯ 14: ĞŸĞĞ’ĞĞ˜Ğ™ Ğ’Ğ˜ĞšĞ›Ğ˜Ğš Ğ”Ğ›Ğ¯ PLAYTECHPEOPLE.COM
============================================================

Ğ¦ĞµĞ¹ Ñ„Ğ°Ğ¹Ğ» Ğ±Ğ°Ğ·ÑƒÑ”Ñ‚ÑŒÑÑ Ğ½Ğ° __test_v.py Ñ‚Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·ÑƒÑ” Ğ¿Ğ¾Ğ²Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑ:
- Ğ¯Ğº Ğ¿Ñ€Ğ°Ñ†ÑÑ” ĞºÑ€Ğ°ÑƒĞ»Ñ–Ğ½Ğ³ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¹Ñ‚Ñƒ
- Ğ¯Ğº Ğ¿Ñ€Ğ°Ñ†ÑÑ” AsyncDriver
- Ğ¯Ğº Ğ¿Ñ€Ğ°Ñ†ÑÑÑ‚ÑŒ URLRule
- Ğ¯Ğº Ğ¿Ñ€Ğ°Ñ†ÑÑ” EdgeCreationStrategy

Ğ’Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ°Ğ½Ğ½Ñ:
    python 14_playtechpeople_trace.py
"""

import sys
import os
import asyncio
import logging
from datetime import datetime
from typing import Optional
from pydantic import Field

# Ğ¨Ğ»ÑÑ… Ğ´Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ñƒ
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
sys.path.insert(0, PROJECT_ROOT)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-7s | %(message)s',
    datefmt='%H:%M:%S'
)

# Ğ’Ğ¸Ğ¼Ğ¸ĞºĞ°Ñ”Ğ¼Ğ¾ Ğ·Ğ°Ğ¹Ğ²Ñ– Ğ»Ğ¾Ğ³Ğ¸
for logger_name in ['urllib3', 'asyncio', 'aiohttp', 'charset_normalizer', 'httpx']:
    logging.getLogger(logger_name).setLevel(logging.WARNING)

logger = logging.getLogger(__name__)


def print_header(title: str):
    print("\n" + "=" * 100)
    print(f"  {title}")
    print("=" * 100)


def print_section(title: str):
    print(f"\n{'â”€' * 80}")
    print(f"  ğŸ“Œ {title}")
    print(f"{'â”€' * 80}")


async def trace_playtechpeople():
    """
    Ğ¢Ñ€Ğ°ÑÑƒĞ²Ğ°Ğ½Ğ½Ñ ĞºÑ€Ğ°ÑƒĞ»Ñ–Ğ½Ğ³Ñƒ playtechpeople.com.
    """
    print_header("Ğ¢Ğ ĞĞ¡Ğ£Ğ’ĞĞĞĞ¯: ĞšĞ ĞĞ£Ğ›Ğ†ĞĞ“ PLAYTECHPEOPLE.COM")
    
    import graph_crawler as gc
    from graph_crawler import AsyncDriver, URLRule
    from graph_crawler.core.models import EdgeCreationStrategy
    
    # ============================================================
    print_section("Ğ•Ğ¢ĞĞŸ 1: ĞšĞĞĞ¤Ğ†Ğ“Ğ£Ğ ĞĞ¦Ğ†Ğ¯")
    # ============================================================
    
    print("""
    ĞšĞ¾Ğ½Ñ„Ñ–Ğ³ÑƒÑ€Ğ°Ñ†Ñ–Ñ ĞºÑ€Ğ°ÑƒĞ»Ñ–Ğ½Ğ³Ñƒ:
    
    URL: https://www.playtechpeople.com/
    max_depth: 3  (Ğ³Ğ»Ğ¸Ğ±Ğ¸Ğ½Ğ° Ğ¾Ğ±Ñ…Ğ¾Ğ´Ñƒ)
    max_pages: 2  (Ğ´Ğ»Ñ ÑˆĞ²Ğ¸Ğ´ĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ñƒ)
    driver: AsyncDriver (aiohttp)
    edge_strategy: NEW_ONLY (Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ Ğ½Ğ¾Ğ²Ñ– Ğ·Ğ²'ÑĞ·ĞºĞ¸)
    
    URL Rules:
    1. /blog/ - ÑĞºĞ°Ğ½ÑƒĞ²Ğ°Ñ‚Ğ¸, Ğ°Ğ»Ğµ ĞĞ• ÑĞ»Ñ–Ğ´ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ¿Ğ¾ÑĞ¸Ğ»Ğ°Ğ½Ğ½ÑĞ¼ (priority=5)
    2. jobs.smartrecruiters.com - ÑĞºĞ°Ğ½ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ· HIGH priority (6)
    """)
    
    # ============================================================
    print_section("Ğ•Ğ¢ĞĞŸ 2: ĞšĞĞ¡Ğ¢ĞĞœĞĞ ĞĞĞ”Ğ Ğ”Ğ›Ğ¯ Ğ¢Ğ•ĞšĞ¡Ğ¢Ğ£")
    # ============================================================
    
    class TextExtractorNode(gc.Node):
        """
        ĞšĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ğ° Ğ½Ğ¾Ğ´Ğ° Ñ‰Ğ¾ Ğ²Ğ¸Ñ‚ÑĞ³ÑƒÑ” Ñ‚ĞµĞºÑÑ‚ ÑÑ‚Ğ¾Ñ€Ñ–Ğ½ĞºĞ¸.
        ĞĞ½Ğ°Ğ»Ğ¾Ğ³ CastomNode Ğ· __test_v.py.
        """
        text: Optional[str] = Field(default=None, description="Ğ¢ĞµĞºÑÑ‚ ÑÑ‚Ğ¾Ñ€Ñ–Ğ½ĞºĞ¸")
        
        def _update_from_context(self, context):
            """
            Ğ’Ğ¸Ñ‚ÑĞ³ÑƒÑ” Ñ‚ĞµĞºÑÑ‚ Ğ· html_tree.
            Ğ’Ğ¸ĞºĞ»Ğ¸ĞºĞ°Ñ”Ñ‚ÑŒÑÑ ĞŸĞ†Ğ¡Ğ›Ğ¯ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ñƒ HTML.
            """
            super()._update_from_context(context)
            
            if context.html_tree:
                # BeautifulSoup API - get_text()
                try:
                    raw_text = context.html_tree.get_text(separator=' ', strip=True)
                    # ĞÑ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ñ
                    clean_text = ' '.join(raw_text.split())
                    clean_text = clean_text.replace('\n', ' ').replace('\t', ' ')
                    self.text = clean_text[:5000]  # ĞĞ±Ğ¼ĞµĞ¶ÑƒÑ”Ğ¼Ğ¾ Ñ€Ğ¾Ğ·Ğ¼Ñ–Ñ€
                    
                    print(f"\n  ğŸ“ TextExtractorNode._update_from_context()")
                    print(f"     URL: {self.url}")
                    print(f"     Ğ¢ĞµĞºÑÑ‚: {len(self.text)} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ–Ğ²")
                except Exception as e:
                    logger.warning(f"Error extracting text: {e}")
                    self.text = None
    
    print("""
    TextExtractorNode ÑƒÑĞ¿Ğ°Ğ´ĞºĞ¾Ğ²ÑƒÑ” gc.Node Ñ‚Ğ° Ğ´Ğ¾Ğ´Ğ°Ñ”:
    
    class TextExtractorNode(gc.Node):
        text: Optional[str] = None
        
        def _update_from_context(self, context):
            # Ğ’Ğ¸Ñ‚ÑĞ³ÑƒÑ”Ğ¼Ğ¾ Ñ‚ĞµĞºÑÑ‚ Ğ· html_tree (BeautifulSoup)
            raw_text = context.html_tree.get_text()
            self.text = clean(raw_text)
    """)
    
    # ============================================================
    print_section("Ğ•Ğ¢ĞĞŸ 3: URL RULES")
    # ============================================================
    
    url_rules = [
        URLRule(
            pattern="/blog/",
            should_follow_links=False,  # ĞĞµ ÑĞ»Ñ–Ğ´ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ¿Ğ¾ÑĞ¸Ğ»Ğ°Ğ½Ğ½ÑĞ¼ Ğ· Ğ±Ğ»Ğ¾Ğ³Ñƒ
            should_scan=True,           # ĞĞ»Ğµ ÑĞºĞ°Ğ½ÑƒĞ²Ğ°Ñ‚Ğ¸
            priority=5
        ),
        URLRule(
            pattern="jobs.smartrecruiters.com",
            should_follow_links=False,
            should_scan=True,
            priority=6  # Ğ’Ğ¸Ñ‰Ğ¸Ğ¹ Ğ¿Ñ€Ñ–Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ Ğ´Ğ»Ñ job ÑÑ‚Ğ¾Ñ€Ñ–Ğ½Ğ¾Ğº
        ),
    ]
    
    print("""
    URL Rules Ğ²Ğ¸Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ñ–:
    
    Rule 1: Blog pages
    â”œâ”€â”€ pattern: "/blog/"
    â”œâ”€â”€ should_scan: True (ÑĞºĞ°Ğ½ÑƒĞ²Ğ°Ñ‚Ğ¸)
    â”œâ”€â”€ should_follow_links: False (Ğ½Ğµ ÑĞ»Ñ–Ğ´ÑƒĞ²Ğ°Ñ‚Ğ¸)
    â””â”€â”€ priority: 5 (default)
    
    Rule 2: Job pages (external)
    â”œâ”€â”€ pattern: "jobs.smartrecruiters.com"
    â”œâ”€â”€ should_scan: True
    â”œâ”€â”€ should_follow_links: False
    â””â”€â”€ priority: 6 (Ğ²Ğ¸Ñ‰Ğ¸Ğ¹)
    
    Ğ•Ñ„ĞµĞºÑ‚:
    - Blog ÑÑ‚Ğ¾Ñ€Ñ–Ğ½ĞºĞ¸ ÑĞºĞ°Ğ½ÑƒÑÑ‚ÑŒÑÑ, Ğ°Ğ»Ğµ Ñ—Ñ… Ğ¿Ğ¾ÑĞ¸Ğ»Ğ°Ğ½Ğ½Ñ Ñ–Ğ³Ğ½Ğ¾Ñ€ÑƒÑÑ‚ÑŒÑÑ
    - Job ÑÑ‚Ğ¾Ñ€Ñ–Ğ½ĞºĞ¸ ÑĞºĞ°Ğ½ÑƒÑÑ‚ÑŒÑÑ Ğ· Ğ²Ğ¸Ñ‰Ğ¸Ğ¼ Ğ¿Ñ€Ñ–Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ¾Ğ¼
    """)
    
    # ============================================================
    print_section("Ğ•Ğ¢ĞĞŸ 4: EDGE CREATION STRATEGY")
    # ============================================================
    
    print("""
    EdgeCreationStrategy Ğ²Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ñ” ÑĞº ÑÑ‚Ğ²Ğ¾Ñ€ÑÑÑ‚ÑŒÑÑ Ğ·Ğ²'ÑĞ·ĞºĞ¸:
    
    1. ALL (default):
       - Ğ¡Ñ‚Ğ²Ğ¾Ñ€ÑÑ” edge Ğ´Ğ»Ñ ĞšĞĞ–ĞĞĞ“Ğ Ğ¿Ğ¾ÑĞ¸Ğ»Ğ°Ğ½Ğ½Ñ
       - A â†’ B, A â†’ C, B â†’ A (Ğ½Ğ°Ğ²Ñ–Ñ‚ÑŒ ÑĞºÑ‰Ğ¾ A Ğ²Ğ¶Ğµ Ñ”)
       - Ğ‘Ñ–Ğ»ÑŒÑˆĞµ edges, Ğ¿Ğ¾Ğ²Ğ½Ğ° ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½Ğ° Ğ·Ğ²'ÑĞ·ĞºÑ–Ğ²
    
    2. NEW_ONLY (Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑ”Ğ¼Ğ¾ Ñ‚ÑƒÑ‚):
       - Edge Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ Ğ´Ğ»Ñ ĞĞĞ’Ğ˜Ğ¥ Ğ½Ğ¾Ğ´
       - A â†’ B (new), A â†’ C (new), B â†’ A (skip, A Ğ²Ğ¶Ğµ Ñ”)
       - ĞœĞµĞ½ÑˆĞµ edges, ĞºÑ€Ğ°Ñ‰Ğ° Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ–ÑÑ‚ÑŒ
    
    3. Ğ’Ğ¸Ğ±Ñ–Ñ€:
       - ALL: Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·Ñƒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸ ÑĞ°Ğ¹Ñ‚Ñƒ
       - NEW_ONLY: Ğ´Ğ»Ñ ĞºÑ€Ğ°ÑƒĞ»Ñ–Ğ½Ğ³Ñƒ Ğ²ĞµĞ»Ğ¸ĞºĞ¸Ñ… ÑĞ°Ğ¹Ñ‚Ñ–Ğ²
    """)
    
    # ============================================================
    print_section("Ğ•Ğ¢ĞĞŸ 5: Ğ—ĞĞŸĞ£Ğ¡Ğš ĞšĞ ĞĞ£Ğ›Ğ†ĞĞ“Ğ£")
    # ============================================================
    
    print("\n  ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº ĞºÑ€Ğ°ÑƒĞ»Ñ–Ğ½Ğ³Ñƒ playtechpeople.com...")
    print("     (max_pages=2 Ğ´Ğ»Ñ ÑˆĞ²Ğ¸Ğ´ĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ñƒ)\n")
    
    start_time = datetime.now()
    
    try:
        graph = await gc.crawl(
            "https://www.playtechpeople.com/",
            max_depth=3,
            max_pages=2,  # ĞĞ±Ğ¼ĞµĞ¶ÑƒÑ”Ğ¼Ğ¾ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ñƒ
            node_class=TextExtractorNode,
            driver=AsyncDriver,
            url_rules=url_rules,
            edge_strategy=EdgeCreationStrategy.NEW_ONLY,
        )
        
        duration = (datetime.now() - start_time).total_seconds()
        
        # ============================================================
        print_section("Ğ•Ğ¢ĞĞŸ 6: Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢Ğ˜")
        # ============================================================
        
        print(f"\n  â±ï¸ Ğ§Ğ°Ñ Ğ²Ğ¸ĞºĞ¾Ğ½Ğ°Ğ½Ğ½Ñ: {duration:.2f} ÑĞµĞºÑƒĞ½Ğ´")
        print(f"  ğŸ“Š Ğ—Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ½Ğ¾Ğ´: {len(graph.nodes)}")
        print(f"  ğŸ”— Ğ—Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ edges: {len(graph.edges)}")
        
        print("\n  ğŸ“‹ Ğ”ĞµÑ‚Ğ°Ğ»Ñ– Ğ½Ğ¾Ğ´:")
        for node_id, node in graph.nodes.items():
            print(f"\n      Node: {node.url[:60]}...")
            print(f"      â”œâ”€â”€ depth: {node.depth}")
            print(f"      â”œâ”€â”€ scanned: {node.scanned}")
            print(f"      â”œâ”€â”€ should_scan: {node.should_scan}")
            print(f"      â”œâ”€â”€ can_create_edges: {node.can_create_edges}")
            
            if isinstance(node, TextExtractorNode) and node.text:
                preview = node.text[:150] + '...' if len(node.text) > 150 else node.text
                print(f"      â””â”€â”€ text preview: '{preview}'")
            
            # Metadata
            if node.metadata:
                title = node.metadata.get('title', 'N/A')
                print(f"      â””â”€â”€ title: {title}")
        
    except Exception as e:
        print(f"\n  âŒ ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ°: {e}")
        import traceback
        traceback.print_exc()
        return None
    
    # ============================================================
    print_section("Ğ•Ğ¢ĞĞŸ 7: ĞĞ Ğ¥Ğ†Ğ¢Ğ•ĞšĞ¢Ğ£Ğ Ğ ĞŸĞ ĞĞ¦Ğ•Ğ¡Ğ£")
    # ============================================================
    
    print("""
    ĞŸĞ¾Ğ²Ğ½Ğ¸Ğ¹ Ğ»Ğ°Ğ½Ñ†ÑĞ¶Ğ¾Ğº Ğ²Ğ¸ĞºĞ»Ğ¸ĞºÑ–Ğ² Ğ´Ğ»Ñ Ñ†ÑŒĞ¾Ğ³Ğ¾ ĞºÑ€Ğ°ÑƒĞ»Ñ–Ğ½Ğ³Ñƒ:
    
    gc.crawl("https://www.playtechpeople.com/", ...)
    â”‚
    â”œâ”€â”€ api/simple.py::crawl()
    â”‚   â”œâ”€â”€ ApplicationContainer()
    â”‚   â”œâ”€â”€ CrawlerConfig(url, max_depth=3, max_pages=2, ...)
    â”‚   â”œâ”€â”€ create_driver(AsyncDriver) â†’ aiohttp driver
    â”‚   â””â”€â”€ GraphCrawlerClient()
    â”‚
    â”œâ”€â”€ client.crawl()
    â”‚   â”œâ”€â”€ CrawlerConfig Ğ· url_rules
    â”‚   â””â”€â”€ GraphSpider(config, driver, ...)
    â”‚
    â””â”€â”€ spider.crawl()
        â”‚
        â”œâ”€â”€ root_node = TextExtractorNode(url, depth=0)
        â”œâ”€â”€ graph.add_node(root_node)
        â”œâ”€â”€ scheduler.add_node(root_node)
        â”‚
        â””â”€â”€ coordinator.coordinate()
            â”‚
            â”œâ”€â”€ [LOOP] while scheduler not empty & pages < max_pages:
            â”‚   â”‚
            â”‚   â”œâ”€â”€ node = scheduler.get_next()  # heapq Ğ·Ğ° priority
            â”‚   â”‚
            â”‚   â”œâ”€â”€ scanner.scan_node(node)
            â”‚   â”‚   â”œâ”€â”€ html = await driver.fetch(url)  # aiohttp GET
            â”‚   â”‚   â”œâ”€â”€ node.process_html(html)
            â”‚   â”‚   â”‚   â”œâ”€â”€ BeautifulSoup.parse(html)
            â”‚   â”‚   â”‚   â”œâ”€â”€ ON_HTML_PARSED plugins
            â”‚   â”‚   â”‚   â”œâ”€â”€ node._update_from_context()  â† TextExtractorNode!
            â”‚   â”‚   â”‚   â””â”€â”€ ON_AFTER_SCAN plugins
            â”‚   â”‚   â””â”€â”€ return extracted_links
            â”‚   â”‚
            â”‚   â””â”€â”€ processor.process_links(node, links)
            â”‚       â”œâ”€â”€ for link in links:
            â”‚       â”‚   â”œâ”€â”€ URLRule matching â†’ priority, should_scan
            â”‚       â”‚   â”œâ”€â”€ if should_scan:
            â”‚       â”‚   â”‚   â”œâ”€â”€ child = TextExtractorNode(link, depth+1)
            â”‚       â”‚   â”‚   â”œâ”€â”€ graph.add_node(child)
            â”‚       â”‚   â”‚   â””â”€â”€ scheduler.add_node(child)  # heapq
            â”‚       â”‚   â””â”€â”€ if edge_strategy == NEW_ONLY:
            â”‚       â”‚       â””â”€â”€ graph.add_edge(node, child)  # Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ Ğ½Ğ¾Ğ²Ñ–
            â”‚       â””â”€â”€ continue loop
            â”‚
            â””â”€â”€ return graph
    """)
    
    print_header("Ğ¢Ğ ĞĞ¡Ğ£Ğ’ĞĞĞĞ¯ Ğ—ĞĞ’Ğ•Ğ Ğ¨Ğ•ĞĞ")
    return graph


if __name__ == "__main__":
    print("\n" + "*" * 100)
    print("  GRAPHCRAWLER v3.0 - Ğ¢Ğ ĞĞ¡Ğ£Ğ’ĞĞĞĞ¯ PLAYTECHPEOPLE.COM")
    print("*" * 100)
    
    graph = asyncio.run(trace_playtechpeople())
    print("\nâœ… Ğ¢Ñ€Ğ°ÑÑƒĞ²Ğ°Ğ½Ğ½Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾ ÑƒÑĞ¿Ñ–ÑˆĞ½Ğ¾!")
